# Score Matching

vic-bayer, October 2024

## Introduction

Score-matching is a method for likelihood maximization applied to models of the form:

pÎ¸(x) = _ğ‘¡ğ‘–ğ‘™ğ‘‘ğ‘’_(x) / ZÎ¸

where _Z Î¸ = âˆ«ğ‘¡ğ‘–ğ‘™ğ‘‘ğ‘’(x) dx_ is generally intractable normalization. Energy models have promise here, approximating the integral via Langevin.

## Problem Setup

Score refers to _âˆ‡ log p(x, Î¸)_. Regularity assumptions assume _âˆ‡ log qâ†’0 |x|_2â†’âˆ_. 

## Energy-Based Models

An example of such a model is an energy-based model which enjoys a very flexible parameterization as a Boltzmann distribution (also called Gibbs distribution) with the form:

pÎ¸(x) = e-fÎ¸(x) / âˆ« e-fÎ¸(x) dx

Here, _f Î¸(x)_ denotes the energy of the distribution in analogy to the Boltzmann distribution. In the context of energy-based model optimization, _âˆ« e -fÎ¸(x) dx_ is often approximated using Monte Carlo methods, or more specifically, Langevin dynamics. This is generally computationally expensive, although in low dimensions, it might be amenable to parallelization via rejection sampling.

## Score Matching Methodology

The idea is that:

âˆ‡x log p(x, Î¸) = âˆ‡x log [epÎ¸(x) / âˆ« epÎ¸(x) dx] = âˆ‡x pÎ¸(x) - âˆ‡x ZÎ¸ = âˆ‡x pÎ¸(x)

In essence, the score function does not depend on the intractable integral while still containing information about the distribution. As the name suggests, score matching aims at matching the gradient of the empirical data density _q(Â·)_ with the gradient of the model density _p(Â·, Î¸)_ , leading to an optimization of the form:

Î¸opt = argminÎ¸ (1/2) Eq ||âˆ‡x log q(x) - âˆ‡x log p(x, Î¸)||22

## Further Analysis

Consider the term:

(1/2) ||âˆ‡x log q(x) - âˆ‡x log p(x, Î¸)||22 = (1/2) (âˆ‡x log q(x))2 \+ 

## Energy-Based Models

An example of such a model is an energy-based model which enjoys a very flexible parameterization as a Boltzmann distribution (also called Gibbs distribution) with the form:

pÎ¸(x) = e-fÎ¸(x) / âˆ« e-fÎ¸(x) dx

Here, _f Î¸(x)_ denotes the energy of the distribution in analogy to the Boltzmann distribution. In the context of energy-based model optimization, _âˆ« e -fÎ¸(x) dx_ is often approximated using Monte Carlo methods, or more specifically, Langevin dynamics. This is generally computationally expensive, although in low dimensions, it might be amenable to parallelization via rejection sampling.

## Score Matching Methodology

The idea is that:

âˆ‡x log p(x, Î¸) = âˆ‡x log [epÎ¸(x) / âˆ« epÎ¸(x) dx] = âˆ‡x pÎ¸(x) - âˆ‡x ZÎ¸ = âˆ‡x pÎ¸(x)

In essence, the score function does not depend on the intractable integral while still containing information about the distribution. As the name suggests, score matching aims at matching the gradient of the empirical data density _q(Â·)_ with the gradient of the model density _p(Â·, Î¸)_ , leading to an optimization of the form:

Î¸opt = argminÎ¸ (1/2) Eq ||âˆ‡x log q(x) - âˆ‡x log p(x, Î¸)||22

## Further Analysis

Consider the term:

(1/2) ||âˆ‡x log q(x) - âˆ‡x log p(x, Î¸)||22 = (1/2) (âˆ‡x log q(x))2 \+ (1/2) (âˆ‡x âˆ‡x log p(x, Î¸))2 \- (1/2) âˆ‡x log q(x) âˆ‡x log p(x, Î¸)

The first term does not depend on _Î¸_ , so it is not relevant for the optimization problem.

## Expectation of the Cross-Term

Now consider:

Eq [-âˆ‡x log q(x) âˆ‡x log p(x, Î¸)] = âˆ« -âˆ‡x log q(x) âˆ‡x log p(x, Î¸) q(x) dx

This simplifies using integration by parts to:

âˆ« -âˆ‡x q(x) âˆ‡x log p(x, Î¸) dx = -limbâ†’âˆ âˆ‡x log p(b, Î¸) q(b) + limaâ†’-âˆ âˆ‡x log p(a, Î¸) q(a) + âˆ« âˆ‡x2 p(x, Î¸) q(x) dx

The last equality follows from integration by parts. Here, HyvÃ¤rinen et al. make the regularity assumptions that:

âˆ‡x log p(x, Î¸) q(x) â†’ 0 when ||x||2 â†’ âˆ

## Limitations and Regularity Assumptions

These assumptions do not hold for general point processes, such as the Poisson process or Hawkes process, which do not necessarily decay sufficiently fast at the boundary. To alleviate this, a weight function enforcing decay can be introduced, as discussed in "Is Score Matching Suitable for Estimating Point Processes?" Furthermore, nominal score matching is not suitable for autoregressive models, which is also addressed in the same work.

## Optimization Problem

Assuming the regularity assumptions hold, the optimization problem can be rewritten as:

Î¸opt = argminÎ¸ Eq [Tr(âˆ‡x2 p(x, Î¸)) - (1/2) ||âˆ‡x log p(x, Î¸)||22]

If the data distribution is in the model class, this optimization yields the optimal value.

