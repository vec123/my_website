<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kalman Filter</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
        }
        p {
            line-height: 1.6;
        }
        .equation {
            text-align: center;
            margin: 20px 0;
        }
        .equation span {
            font-family: "Courier New", monospace;
            background-color: #f9f9f9;
            display: inline-block;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
        }
        .highlight {
            color: red;
        }
        .author-date {
            font-style: italic;
            margin-bottom: 20px;
        }
    </style>

<script type="text/javascript" id="MathJax-script" async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>

    <h1>Kalman Filter</h1>

    <h2>Introduction</h2>

    <p> Let us look at the Kalman filter. 
        We assume, a noisy linear system with covariance matrix $Q$ and 
        that noisy measurements with covariance matrix $R$:</p>
    <div class="equation">
        $$
        x_t = F x_{t-1} + Bv_{t-1}
        $$
        $$
        y_t = H x_t + Dw_t
        $$
    </div>
    <p>where <i>v_k</i> and <i> w_k</i> are independent gaussian noise terms with zero mean and isotropic covariance. Consider <i>\( \mathbb{E}(x_0) = \mu_0 \)</i> 
        and  <i>\( \mathbb{Var}(x_0) = \Sigma_0 \).</i> </p>
    Let us look at the time evolution of this mean and covariance.</p>
    <div class="equation">
        $$
        \mu_{t+1}  = F \mu_{t} 
        $$
        $$
        \Sigma_{t+1} = F \Sigma_{t} F^T + BB^T
        $$
    </div>
    <p>For <i>y</i> the evolution can be obtained analogous. Note that 
    </p>
    <div class="equation">
        $$
        \lim_{t \rightarrow \infty} \Sigma_{t} =  \lim_{t \rightarrow \infty}  F \Sigma_{t-1} F^T + BB^T
        $$
        $$
        =  \lim_{t \rightarrow \infty} F^t \Sigma_0 \left(F^t\right)^T + \sum_{k=0}^{t-1} F^{k} BB^T \left(F^k\right)^T
        $$
        $$
        = \lim_{t \rightarrow \infty}  \sum_{k=0}^{t-1} F^{k} BB^T  \left(F^k\right)^T
        $$
    </div>
    <p> since <i>F</i> is assumed stable (i.e. it has eigenvalues stricly smaller 0). In the limit we have</p>
    <div class="equation">
        $$
        \Sigma_{\infty} = F  \Sigma_{\infty}  F^T + BB^T
        $$
    </div>
    <p> This equation is stricly positive and a discrete time Lyapunov Equation. If \(F\) is stable, it has a unique solution <i>\( \Sigma_{\infty} \)</i></p>
    <p>Consider now <i> B = I</i> and <i> D = I</i>, and assume the noise terms are zero mean with covariance </p>
    </p>
    <div class="equation">
        $$
        \text{VAR} \begin{bmatrix}
        v \\
        w
         \end{bmatrix}
         =
         \begin{bmatrix}
        Q & S \\
        S^T & R
         \end{bmatrix}
        $$
    </div>
    <p>The state and measurement noises are uncorrelated at different times but at time <i>t</i> correlation can exist. 
        We can deconstruct <i>v</i> into a term correlated to <i>w</i> and one uncorrelated to it. We obtain </p>
    <div class="equation">   
        $$
     \tilde{v}(t) = \mathbb{E}[v(t)|w(t)] = v(t) - S R^{-1} w(t)  = v(t) -  S R^{-1}y(t) +  S R^{-1} C x(t)
     $$
    </div>
    <p>by the conditional mean of a gaussian. Thus</p>
    <div class="equation">   
        $$
        x(t+1) = (F - S R^{-1} C) x(t) +  S R^{-1}y(t) +  \tilde{v}(t)
        $$
        $$
        y(t) = Cx(t) + w(t)
        $$
    </div>
 <p>with</p>
    <div class="equation">
        $$
        \text{Var}( \begin{bmatrix}
        \tilde{v} \\
        w
        \end{bmatrix})
        =
        \begin{bmatrix}
        \tilde{Q} & 0 \\
        0 & R
        \end{bmatrix}
        $$.
    </div> 
    <p>where <i>\( \tilde{Q} = Q - SR^{-1}S^T \)</i>. These are the starting equations for the Kalman Filter. 
    We want to obtain a predictor <i>\( \hat{x}(t+1|t) = \mathbb{E}(x(t+1)|H_t(y)) \)</i>, filter <i>\( \hat{x}(t|t)= \mathbb{E}(x(t+1)|H_{t+1}(y)) \)</i>. 
    The predictor error is  <i>\( e(t+1|t) = x(t+1) - \hat{x}(t+1|t) \)</i>
    and its covariance is
    <i>\(
        P(t+1|t) =\mathbb{E}[e(t+1|t)e(t+1|t)^T]
        \)
    </i>. 
    Analogues can be obtained for the filter.
    We call  <i>\( l(t) = y(t) - C \hat{x}(t|t-1) = y(t) - \hat{y}(t) \)</i> the innovation. It can be realted to how much we trust the state-predictor given the measurements. 
    Of course, state and measurements are subject to noise.
    We have introduced <i>\( H_{t}(y) \)</i> which corresponds to an aggregation of the masurement information at time <i>t</i>. 
    This allows a representation in terms of a Markov chain, where the next state only depends on the current observation.
    Consider the time update of the predictor mean and its error covariance
</p>
    <div class="equation">
    $$
    \hat{x}(t+1|t) = F \hat{x}(t) + S R^{-1}y(t)
    $$
    $$
    P(t+1|t) = F P(t|t) F^T + \tilde{Q}
    $$
    </div>
<p> and the measurement update </p>
    <div class="equation">
        $$
        \hat{x}(t+1|t+1) = \hat{x}(t+1|t)  + L(t+1)l(t+1)
        $$
        $$
        l(t+1) = y(t+1)- C \hat{x}(t+1|t)
        $$
    </div>
    <p>
        where
    </p>
    <div class="equation">
        $$
        L(t+1) = P(t+1|t)C^T \Delta^{-1} (t+1)
        $$
        $$
        \Delta(t) = \text{Var} (l(t)) = CP(t|t-1)C^T +R
        $$
        $$
        P(t+1|t+1) = P(t+1|t) - P(t+1|t)C^T \Delta^{-1} (t+1)CP(t+1|t)
        $$
    </div>
    <p>
        Find a proof <a href="#proofs">here</a>. We can summarize these equations 
    </p>
    <div class="equation">
        $$
        \hat{x}(t+1|t) = F \hat{x}(t|t-1) + SR^{-1}y(t) + K(t)(y(t) - C \hat{x}(t|t-1))
        $$
        $$
        K(t) = F L(t) = F P(t|t-1)C^T \Delta^{-1}(t)
        $$
        $$
        \Delta^{-1}(t) = \text{Var}( l(t) )
        $$
        $$
        P(t+1|t) = F[ P(t|t-1) - P(t|t-1)C^T \Delta^{-1}(t)CP(t|t-1)]F^T + Q
        $$
        $$
        P(t_0|t_0-1) = P_0
        $$
    </div>
    <p> This can be interpreted as a system with state-feedback which is fed by the innovation \(l(t) = y(t+1)- C \hat{x}(t+1|t) \).
        Note, that</p>
    <div class="equation">
        $$
        \hat{x}(t+1|t) = (F - K(t)C)\hat{x}(t|t-1) + (SR^{-1}+K(t))y(t) = \Gamma(t)\hat{x}(t|t-1)+ (SR^{-1}+K(t))y(t)
        $$
        and
        $$
        x(t+1) - \hat{x}(t+1|t) = \Gamma(t)(\hat{x}(t|t-1)) - K(t)w(t) + \tilde{v}(t)
        $$
    </div>
    <p> where <i>\( \Gamma(t) = F - K(t)C \)</i> determines the error dynamics.</p>
    <div class="equation">
        $$
        P(t+1|t) = \Gamma(t)P(t|t-1)\Gamma(t)^T + K(t)RK(t)^T + \tilde(Q)
        $$
        $$
        P(t_0|t_0-1) = P_0
        $$
    </div>

    <p>To simplify we assume no noise correlation, i.e \(S = 0 \), leading to the simplified version</p>
 
    <div class="equation">
        $$
        \hat{x}(t+1|t+1) = F \hat{x}(t|t) + P(t+1|t)C^T \Delta^{-1}(t+1)(y(t+1) - CF\hat{x}(x|x)) 
        $$
        $$
        \Delta(t+1)= F L(t) = C P(t+1|t)C^T+R
        $$
        $$
        P(t+1|t+1) = [I - P(t+1|t) C^T \Delta^{-1} (t+1)C]P(t+1|t)
        $$
    </div>
    <p> Note that </p>
    <div class="equation">
        $$
        P(t+1|t) = FP(t|t-1)F^T -  FP(t|t-1)C^T \Delta^{-1} (t)CP(t|t-1)F^T + Q
        $$
    </div>
    <p>and</p>
    <div class="equation">
        $$
        \hat{x}(t+1|t) = (F - K(t)C)\hat{x}(t|t-1) + Ky(t) = \Gamma(t)\hat{x}(t|t-1)+ K(t)y(t)
        $$
    </div>
    <p> 
        are especially important equations. They can be used to determine the asymptiotic behaviour of \( \lim_{t \rightarrow \infty}P(t+1|t) \) and \( \lim_{t \rightarrow \infty}\hat{x}(t+1|t) \).  
        Recall that we have seen that, in the limit, the covariance of the process is determined by
        <div class="equation">
            $$
            \Sigma = F \Sigma F^T + Q
            $$
        </div>
        If \( F \) is stable, then \( x(t) \) converges to a stationary value and becomes a stationary process. 
        If this is the case, then <i>P(t)</i> also converges. For example if <i>K = 0</i> (no measurement data is avaialable),
        <i> \(\lim_{t \rightarrow \infty} \hat{x}(t+1|t) = 0 = \lim_{t \rightarrow \infty}  \mathbb{E}x(t) \)</i> and <i> \( \lim_{t \rightarrow \infty}P(t) = \Sigma \).</i>
        If <i>\( \Gamma \)</i> is unstable, then <i>\( \hat{x} \)</i> is non stationary.  Stability of <i>\( F \)</i> is a sufficient but not a necessary criterion for predictor convergence.
        It is also possible to have predictor convergence and/or bounds with an unstable system. 
        This predictor convergence of a non stationary state evolution is made possible by an equally non stationary measurement <i>\( y(t) \)</i> 
        which feeds the predictor. This can be shown on the simple system 
        <div class="equation">
            $$
            x(t+1) = \alpha x(t) + w(t)
            $$
            $$
            y(t) = \gamma x(t) + v(t)
            $$
        </div>
        with <i>\( w(t) \sim \mathcal{N}(0, \beta) \)</i> and <i>\( v(t) =  \sim \mathcal{N}(0, 1) \)</i>.
        We obtain 
        <div class="equation">
            $$
            P(t+1) = \beta^2 + \frac{\alpha^2P(t)}{1 + \gamma^2P(t)}.
            $$
        </div>
        This equation has a stable solution if <i>\( \gamma > 0 \)</i>. If <i>\( \gamma = 0 \)</i> then <i>P(t)</i> diverges. If <i>\( \beta = 0 \)</i>,
        i.e. the state evolution is not noisy, then there exist two solutions. One is stable and one is unstable. The unstable solution is at <i>P(t) = 0</i>.
        <i>\( \gamma \neq 0 \)</i>  and <i>\( \beta \neq 0 \)</i> ensure  the existence of a unique solution satisfying the convergence of <i>P(t)</i> to a stabilizing covariance.
        We have examined a simple system with one dimension only. Translated into a higher dimensional system, the same principles apply. We have
        <div class="equation">
            $$
            x(t+1) = A x(t) + Bw(t)
            $$
            $$
            y(t) = C x(t) + Dv(t)
            $$
        </div>
        with <i>\( w(t) \sim \mathcal{N}(0, I) \)</i> and <i>\( v(t) =  \sim \mathcal{N}(0, I) \)</i> or <i>\( Bw(t) \sim \mathcal{N}(0, BB^T = Q) \)</i> 
        and  <i>\( Dv(t) \sim \mathcal{N}(0, DD^T = R) \)</i>.
        If <i>(A,C)</i> is observable (no distinct <i>x(0)</i> generate the same outputs), 
        then there exists at elast one solution for the covariance <i>\( \lim_{t \rightarrow \infty} P(t) \)</i>.
        There could be more than one solution (as in the case <i>\( \beta = 0 \)</i> and <i>\( \gamma \neq 0 \)</i> ).
        If <i>(A,B)</i> is controllable (equivalent to controllable <i>(A,Q)</i>), then the solution is unique and stabilizing.
        These notions can be relaxed to detectability and stabilizability. In essence, if <i>(A,B)</i> is stabilizable and <i>(A,C)</i> is detectable 
        (i.e. all uncontrollable and unobservable modes are asymptotically stable),
        then there exists a unique stabilizing solution for the covariance <i>\( \lim_{t\rightarrow \infty}P(t) \)</i>.
        Find the proof <a href="#proofs">here</a>.
    </p>
    <h2 id="proofs">Proofs</h2>
    <h3>Time and Measurement Update</h3>
    <p>Let us look at the time update of the predictor mean and its error covariance</p>
</p>
    <div class="equation"> 
    $$
    \hat{x}(t+1|t) = F \hat{x}(t) + S R^{-1}y(t)
    $$
    $$
    P(t+1|t) = F P(t|t) F^T + \tilde{Q}
    $$
    </div>
    <p> and the measurement update </p>
    <div class="equation">
        $$
        \hat{x}(t+1|t+1) = \hat{x}(t+1|t)  + L(t+1)l(t+1)
        $$
        $$
        l(t+1) = y(t+1)- C \hat{x}(t+1|t)
        $$
    </div>
    <p>
        where
    </p>
    <div class="equation">
        $$
        L(t+1) = P(t+1|t)C^T\Delta^{-1}(t+1)
        $$
        $$
        \Delta(t) = \text{Var}( l(t) ) = CP(t|t-1)C^T +R
        $$
        $$
        P(t+1|t+1) = P(t+1|t) - P(t+1|t)C^T\Delta^{-1}(t+1)CP(t+1|t)
        $$
    </div>
        
    <p> Consider </p>
    <div class="equation">
        $$
        x(t+1) = F x(t) + S R^{-1}y(t) + \tilde v(t)
        $$
        $$
        y(t+) = Cx(t) + \tilde w(t)
        $$
    </div>
    <p> and note that <i>v(t)</i> is independent from <i>y(t)</i>. Thus <i>\( \mathbb{E}(x(t+1)|H_{t+1}(y)) = \hat{x}(t+1|t) =  F \hat{x}(t|t) + S R^{-1}y(t) \)</i> 
    and therefore
    <div class="equation">
        $$
        e(t+1|t) = x(t+1) -\hat{x}(t+1|t) = Fx(t) + \tilde{v}(t) + S R^{-1}y(t) - F\hat{x}(t|t) -  S R^{-1}y(t)
        $$
        $$
        = F( x(t) - \hat{x}(t|t) ) +  \tilde{v}(t) = F  e(t+1|t)  +  \tilde{v}(t)
        $$
    </div>
    and
    <div class="equation">
        $$
        P(t+1|t) = F P(t|t)F^T +\tilde{Q} = \text{Var}(e(t+1|t)) = F \text{Var}(e(t|t))F^T + \text{Var}(\tilde(v)(t))
        $$
    </div>
    Furthermore, consider the measurement update and the fact that the space spanned by<i>l(t)</i> is orthogonal to the space spanned by <i>y(t)</i> (by definition).
    Thus <i>\( H_{t+1}(y) = H_{t}(y)+ H(l(t+1)) \)</i>. Thus
    <div class="equation">
        $$
       \mathbb{E}(x(t+1)|H_t(y)) = \mathbb{E}(x(t+1)|H_t(y)) + \mathbb{E}(x(t+1)|H(l(t+1))) = \hat{x}(t+1|t) + \text{cov}(x(t+1,l(t+1)))+\text{var}(l(t+1))^{-1}l(t+1)
        $$
        $$
        = \hat{x}(t+1|t) + P(t+1|t)C^T(CP(t+1|t)C^T + R)^{-1} l(t+1) =  \hat{x}(t+1|t+1) = \hat{x}(t+1|t)  + L(t+1)l(t+1)
        $$
    </div>
    by the conditional of a gaussian. Updating the covariances is analogous:
    <div class="equation">
    $$
       e(t+1)= x(t+1) - \hat{x}(t+1|t+1) = x(t+1) -  \hat{x}(t+1|t) - L(t+1)l(t+1) = e(t+1) - L(t+1)l(t+1)
    $$
    </div>
    leading to 
    <div class="equation">
        $$
           e(t+1|t+1) +  L(t+1)l(t+1) = e(t+1|t)
        $$
    </div>
    with variance
    <div class="equation">
        $$
           P(t+1|t+1) + L(t+1) \text{var}(l(t+1))^{-1} L(t+1)^T = P(t+1|t) = P(t+1|t+1) + L(t+1) \Delta^{-1} L(t+1)^T
        $$
    </div>
    This concludes the proofs.
    </p>

    <h3>Convergence and Stability of the predictor</h3>

    </body>
    </html>
        