<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
        }
        p {
            line-height: 1.6;
        }
        .equation {
            text-align: center;
            margin: 20px 0;
        }
        .equation span {
            font-family: "Courier New", monospace;
            background-color: #f9f9f9;
            display: inline-block;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
        }
        .highlight {
            color: red;
        }
        .author-date {
            font-style: italic;
            margin-bottom: 20px;
        }
        .math-display {
            text-align: center;
            font-size: 1.2em;
            margin: 20px 0;
        }
        pre {
            background: #f0f0f0;
            padding: 10px;
            overflow: auto;
        }
    </style>
</head>
<body>

    <h1>Wasserstein distance and algorithms</h1>

    <h2>Introduction</h2>
    <p>The Wasserstein distance is a distance between shapes based on optimal transport.
        Consider two probability densities \(p(x)\) and \(q(x)\) defining the distributions \(P\) and \(Q\), 
        so that \( x \sim P \) and \( y \sim Q \) with \( x,y \in \mathbb{R}^d \).
        Its original formulation is the Monge-Kantorovich formulation which considers the optimal transport distance as
        <div class="equation">
           $$
        \inf_T \int || x - T(x) ||^p dP(x).
            $$.
        </div>
        Here \( dP(x) \) is the probability measure and \( T(x) \) is the transport map. 

        If the minimizer  \( T^*(x) \) exists it is the optimal transport map.
        However, in this formulation, a solution is not guaranteed to exist. 
        For example, if \(p(x)\) corresponds to a point-mass and \(q(x)\) corresponds to multiple point-masses,
        then, due to the usage of equal probability measure for both distributions, the mass can not be splitted. 
        An alternative formulation is the soft formulation
        <div class="equation"  id="eq-wasserstein">
            $$
            W_p(P,Q) = (\inf_{J\in \mathcal{J}(P,Q)} \int_{x,y} ||x-y||^p dJ(x,y) )^\frac{1}{p}
             $$
         </div>
        so that \( \int_y J(x,y) dy = P $ and $\int_x J(x,y) dy = Q \).
        The minimizer \( J^*(\cdot) \) is called the optimal transport plan and is guaranteed to exist.
        If \(T: X \rightarrow X\), i.e. no splitting of mass occurs (input and output measure coincide), 
        it is called the optimal transport map. 
        For many applications the input and output measures are equal.
        Note that minimizing the Wasserstein distance returns the minimal cost, as well as a corresponding transport plan. 
        This makes it a very attractive method for image morphing, image registration and further. 
        For example, consider two greyscale images in \(2D\). 
        Then the optimal transport map is a \(2D\) vector-field, where each point receives a \(2D\)-vector with the corresponding displacement. 
        In a discrete setting we have samples \( x_i \) and \( y_j \) which occur with probability \( p_i \) and \( q_j \) respectively.
        Equation <a href="#eq-wasserstein">Wasserstein</a> can be written as
        <div class="equation" id = "eq-wasserstein discrete">
        $$
        W_p(P,Q) = \min_{J\in \mathcal{J}(P,Q)} \sum_i \sum_j ||x_i-y_j||^p J(x_i,y_j)
        $$
        </div>
        so that \( \sum_j J(x_i, y_j) = p_i \), \() \sum_i J(x_i, y_j) = q_j \) and \( J(x_i,y_j) \geq 0 \).
        <br><br>
        Interesting cases are  \( p=1 \) (also called Earth-Mover distance) and \( p=2 \).
        For \( p=2 \), it can be shown that there exists a unique convex function \( \phi \) so that \( J^*(\cdot) = \nabla \phi \).
        Furthermore, for \( p= 2\) the metric space $ \{ P(\Omega), W_p \}$ has the structure of a Riemannian manifold.
        This might also hold for other values of \( p \) 
        (see more <a href="https://link.springer.com/article/10.1007/s41884-018-0014-4">Wasserstein Riemannian geometry of Gaussian densities
        </a>)
        Equation <a href="#eq-wasserstein">Wasserstein</a> can be written in a dual formulation as 
        <div class="equation" >
            $$
            W_p(P,Q) = (\sup_{\Phi, \Psi} \int_{y} \Phi(y)dQ(y) - \int_{x} \Psi(x)dP(x)),
            $$
        </div>
        For \( p=1 \) this can be transformed into 
        <div class="equation">
            $$
            W_p(P,Q) = (\sup_{f} \int_{x} f(x)dQ(x) - \int_{x} f(x)dP(x) ),
            $$
        </div> which is the loss-term employed in Wasserstein GANs.
        Another important fact is that the computing the Wasserstein is expensive, unless the probabilities \( p(x) \) and \( q(x) \) are one dimensional.
        In the one-dimensional case the distance has a closed form solution. 
        <br><br>
        A notational pecularity is  \( T_{\#}P \), which corrsponds to a new probability distribution induced by \( P \) through the mapping \( T \). 
        Consider the optimal transport map \( T:  \mathbb{R}^d \rightarrow  \mathbb{R}^d \), 
        where the distribution of \( T(X) \) is called push-forward ( \( T_{\#}P \) ) of \(P\), so that
        <div class="equation">
            $$
        T_{\#}P (A) = P(\{ x: T(x) \in A \}) = P(T^{-1}(A)).
        $$
        </div> This is also called the push-forward (T_{\#}P) of $P$.
        </p>
        <p> 
        <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf">Generalized Sliced Wasserstein Distances</a>
        presents a way of computing the Wasserstein distance in a more efficient way by using a sliced approximation.  
        <a href="https://arxiv.org/abs/2410.12176">Expected Sliced Transport Plans</a>
        examines how an expected transport plan can be obtained by the sliced computation.
        This is useful for high-dimensional distributions where exact numerical computation can be too expensive. 
        By representing shapes and images as probability distributions, one can use the Wasserstein distance to compare shapes, compute mean shapes
        and interpolate with transport plans.

        Sources: 
        <br><br>
        <a href="https://stat.cmu.edu/~larry/=sml/Opt.pdf">Optimal Transport and Wasserstein Distance - Carnegie uni</a>,
        <br><br>
        <a href="https://ieeexplore.ieee.org/document/7974883">Optimal Mass Transport - Signal processing and machine-learning applications</a>,
        <br><br>
        <a href="https://arxiv.org/abs/1804.01947">Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative Model</a>,
        <br><br>
        <a href="https://link.springer.com/book/10.1007/978-3-030-38438-8">An Invitation to Statistics in Wasserstein Space</a>,
        <br><br>
        <a href="https://link.springer.com/book/10.1007/978-3-319-20828-2">Optimal Transport for Applied Mathematicians</a>,
        <br><br>
        <a href="https://link.springer.com/book/10.1007/978-3-319-20828-2">Optimal Transport for Applied Mathematicians</a>,
        <br><br>
        <a href="https://ieeexplore.ieee.org/document/7780937">Sliced Wasserstein Kernels for Probability Distributions</a>,
        <br><br>
        <a href="https://arxiv.org/abs/1711.05376">Sliced Wasserstein Distance for Learning Gaussian Mixture Models</a>,
        </p>

        </body>
        </html>
        