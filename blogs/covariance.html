<!DOCTYPE html>
<html lang="en">
<head>
    <style>
        .figure-container {
            display: flex;
            justify-content: space-around;
        }

        .small-image {
            width: 400px; /* Adjust this as desired */
        }

        .medium-image {
            width: 120px; /* Adjust this as desired */
        }
    </style>

    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>On the Covariance</title>
</head>


<body>

<h1>On the Covariance</h1>

<p>As an informative description of a linear data distribution, the covariance matrix is an interesting object. Here I try to give some intuitions on its geometry.</p>

<p>As a square matrix, the covariance forms a linear vector space. The special case of symmetric matrices is a linear vector subspace of the linear space of square matrices. A matrix is inherently a linear operator and cannot capture non-linear dependencies by itself.</p>

<p>A linear vector-space is a manifold. In this case, the manifold is flat, meaning it has no curvature and can be globally parameterized. A linear vector-space can be equipped with the Euclidean metric, which defines the distance between two points as the length of the straight-line. For a flat space, this corresponds to the geodesic distance associated with the metric. It coincides with the euclidean distance. A manifold is a topological space that locally resembles Euclidean space. A linear vector-space resembles Euclidean space locally and globally.</p>

<p>The set of symmetric matrices forms a linear vector-space. All open neighborhoods of a manifold are also a manifold. Each positive definite symmetric matrix has another positive definite symmetric matrix in its open neighborhood. Thus, the set of symmetric positive definite matrices forms a manifold. The same does not hold for semi-positive definite matrices.</p>

<p>We say that a covariance matrix induces a regular distribution if it is positive definite and a singular distribution if it is semi-positive definite. If it is semi-positive definite, then there exists a feature-dimension/direction along which no data variability occurs. If one measures data with a singular covariance, the singular dimension carries no information on the data-variability and can be omitted without loss of description accuracy. If one were to examine the likelihood of a model for describing data, the singular dimension has no influence on the likelihood.</p>

<p>This gives us a basis for understanding distributions as being located on manifolds (i.e., we can move smoothly from one distribution, defined by covariance and mean, to another), and a (vector-) directional sense in terms of model parameters/feature dimensions with influence on data-variability and model-likelihood.</p>

<p>We start by considering data distributions and extend this to model-distributions. The concepts are transferable. In one case, each sample corresponds to a data-sample (defined by the feature dimensions/parameters), in the other case, each sample corresponds to a model-sample (defined by the model dimension/parameters). This can be related to the difference between model parametrizations vs. model-free parametrizations of data-distributions.</p>





<h3>Some covariances and their distributions </h3>
<p>The simplest distribution is white noise, which in \(2D\) corresponds to a Gaussian with mean at the origin and covariance matrix:</p>

<div class="equation-block">
$$
\Sigma = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$
</div>


<p>Note that it is regular. White noise can be transformed to another (linear) data-distribution by a linear transformation. Let:</p>
<div class="equation-block">
$$
z \sim \mathcal{N}(0, I)
$$
</div>

<p>be sampled from white noise. Then</p>
<div class="equation-block">
$$
x = A z + b
$$
</div>

<p>has covariance</p>
<div class="equation-block">
$$
\Sigma_x = A I A^T = A A^T
$$
</div>

<p>and mean</p>
<div class="equation-block">
$$
\mu_x = b.
$$
</div>

<p>The covariance matrix can be diagonalized. The eigendecomposition of \(\Sigma\) gives:</p>
<div class="equation-block">
$$
\Sigma = U D U^T
$$
</div>

<p>where \( U \) is a matrix of eigenvectors, and \( D \) is a diagonal matrix of eigenvalues.</p>

<p>Set \( A = U D^{1/2} \), where \( D^{1/2} \) is the square root of the diagonal eigenvalue matrix. The transformation then becomes:</p>
<div class="equation-block">
$$
x = U D^{1/2} z + b
$$
</div>

<p>giving the desired covariance. In the following, the distributions which can be described in this way are visualized.</p>
<h3>Linear Distributions</h3>
<p>Let us visualize some distributions.</p>

<div>
    <figure>
        <figcaption>Figure 1: Linear Distributions</figcaption>
        <div style="display: flex; justify-content: space-around;">
            <figure style="flex: 0 0 10%;">
                <img src="../images/covariance/data_images/white_noise.png" alt="White Noise" class="small-image">
                <figcaption>White Noise</figcaption>
            </figure>
            <figure style="flex: 0 0 10%;">
                <img src="../images/covariance/data_images/specific_covariance.png" alt="Transformation to the same mean, different covariance" class="small-image">
                <figcaption>Transformation to the same mean, different covariance</figcaption>
            </figure>
        </div>
    </figure>
</div>

<p>If we want the mean of our transformed distribution to follow the line:</p>
<div class="equation-block">
$$
y = a \cdot x + b.
$$
</div>


<p>We introduce noise with variance \( c \) in the normal direction to the line. The line \( y = a \cdot x + b \) has a slope vector \( (1, a) \). The normal direction to this slope is given by:</p>
<div class="equation-block">
    $$
\vec{n} = (-a, 1).
$$
</div>

<p>The normalized normal vector is:</p>
<div class="equation-block">
    $$
\hat{n} = \frac{(-a, 1)}{\sqrt{1 + a^2}}.
$$
</div>

<p>Scale the noise to have variance \( c \):</p>
<div class="equation-block">
    $$
\sqrt{c} \cdot \epsilon \sim N(0, c).
$$
</div>

<p>Project the scaled noise along the normalized normal direction:</p>
<div class="equation-block">
    $$
\delta = \sqrt{c} \cdot \epsilon \cdot \hat{n} = \sqrt{c} \cdot \epsilon \cdot \frac{(-a, 1)}{\sqrt{1 + a^2}}.
$$
</div>

<p>Combine the mean and the noise to get the final distribution:</p>
<div class="equation-block">
    $$
y = a \cdot x + b + \sqrt{c} \cdot \epsilon \cdot \frac{(-a, 1)}{\sqrt{1 + a^2}}.
$$
</div>
<p>To shift the distribution from a mean of \( a \cdot x + b \) to a mean of \( c \cdot x + b \) a rotation matrix can be applied. Essentially,</p>
<div class="equation-block">
    $$
\mu' = R \mu
$$
</div>

<p>and</p>
<div class="equation-block">
    $$
\Sigma' = R \Sigma R^T.
$$
</div>

<p>For \( c \cdot x + d \), an offset has to be included so that</p>
<div class="equation-block">
    $$
y = a \cdot x + b + \sqrt{c} \cdot \epsilon \cdot \frac{(-a, 1)}{\sqrt{1 + a^2}} + (d - b).
$$
</div>

<p>Note that if \( cx + d = 0 \),</p>
<div class="equation-block">
    $$
y = \sqrt{c} \cdot \epsilon \cdot \frac{(-a, 1)}{\sqrt{1 + a^2}}
$$
</div>

<p>and we return to a distribution with a certain covariance around mean zero.</p>

<p>Another way of representing the distributions along lines is by the mentioned transformation of white noise so that the variance in the desired direction is very high, potentially infinite. Thus, linear distributions (as I call them) are equivalent to a transformation of white noise.</p>

<p>Note that if the mean follows a non-linear function (e.g., a sine), we could still create this distribution, following the reasoning above, by determining the normal vector and applying noise in that direction. However, due to dependencies on \( x \), this is not an affine transformation of white noise.</p>

<h3>Regular and Singular Covariance</h3>
<p>Figures below show distributions that can be obtained by a rotation. Both can be obtained from white noise. One direction has a very high variance. This variance would potentially be infinite, but in reality, most real-world data is finite.</p>

<div>
    <figure>
        <figcaption>Figure 2: Regular Covariance with Linear Mean</figcaption>
        <div style="display: flex; justify-content: space-around;">
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/uniform_line (regular distribution).png" alt="Regular covariance with linear mean" class="small-image">
                <figcaption>Regular covariance with linear mean</figcaption>
            </figure>
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/transformed_uniform_line (regular distribution).png" alt="Regular covariance with linear mean" class="small-image">
                <figcaption>Regular covariance with linear mean</figcaption>
            </figure>
        </div>
    </figure>
</div>

<p>An important example is the distributions in the figures below, where the variance normal to the line is zero. The data is two-dimensional, yet only one dimension varies. The covariance matrix does not have full rank and corresponds to a singular distribution. The data can be represented just as well in a single dimension without loss of accuracy.</p>

<div>
    <figure>
        <figcaption>Figure 3: Singular Covariance</figcaption>
        <div style="display: flex; justify-content: space-around;">
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/uniform_line (singular distribution).png" alt="Singular covariance" class="small-image">
                <figcaption>Singular covariance</figcaption>
            </figure>
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/transformed_uniform_line (singular distribution).png" alt="Singular covariance" class="small-image">
                <figcaption>Singular covariance</figcaption>
            </figure>
        </div>
    </figure>
</div>

<p>If we measure data with a singular covariance, some directions are degenerate (have zero variance). This is an indication that those directions provide no information, and these dimensions can be omitted without losing description accuracy.</p>
<h3>Mahalanobis Distance and Covariance Geometry</h3>
<p>Within the linear space that contains our distribution, one can define the Mahalanobis distance as:</p>
<div class="equation-block">
    $$
d_M(x, \mu) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}.
$$
</div>

<p>In directions where the data has high variance, the Mahalanobis distance will be smaller. Conversely, in directions of low variance, the distance will be larger. This accounts for the fact that outliers in low-variance directions are more significant than in high-variance directions.</p>

<p>In light of this, the covariance might be interpreted as a deformation of the linear space, which stretches some directions (those with low variance) and compresses others (those with high variance). 
    While it can introduce diagonal relationships, no curvature or torsion can be created by a constant covariance matric.
    If \(\Sigma = I\) (as for white noise), the Mahalanobis distance coincides with the Euclidean distance, and \(\Sigma = I\) might be interpreted as uniformly "stretched" space. The elements of \(\Sigma^{-1}\) correct for the stretching and compression of space by accounting for the effect of variance and covariance.</p>

<h3>Distributions with Constant Mean</h3>
<p>The distributions we considered in the figures above have a constant mean. These distributions can be obtained from white noise by linear transformations. All regular distributions can be transformed back to white noise, whereas the two singular distributions cannot.</p>

<p>In some cases, \(\mu(x,y)\) is nonlinear, but the covariance is constant.</p>

<div>
    <figure>
        <figcaption>Figure 4: Sinusoidal Mean and Constant Variance</figcaption>
        <div style="display: flex; justify-content: space-around;">
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/sine_const_var.png" alt="Sinusoidal Mean and constant variance" class="small-image">
                <figcaption>Sinusoidal Mean and constant variance</figcaption>
            </figure>
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/circle_const_Var (2).png" alt="Circular Mean and constant variance" class="small-image">
                <figcaption>Circular Mean and constant variance</figcaption>
            </figure>
        </div>
    </figure>
</div>

<h3>Distributions with Varying Covariance</h3>
<p>Another interesting case is when the data distribution is not described by a constant covariance matrix. Instead, the covariance is a function of the parameters. We can distinguish between cases where this function is linear vs. non-linear. Since this relationship is not a constant, it requires a parameterized covariance matrix of the form:</p>
<div class="equation-block">
    $$
\Sigma(x,y) = \begin{bmatrix}
f_{x,x}(x,y) & f_{x,y}(x,y) \\
f_{y,x}(x,y) & f_{y,y}(x,y)
\end{bmatrix}.
$$
</div>

<p>In most cases, we will consider only continuous functions for the elements of \(\Sigma(x,y)\).</p>

<div>
    <figure>
        <figcaption>Figure 5: Varying Covariance</figcaption>
        <div style="display: flex; justify-content: space-around;">
            <figure style="flex: 0 0 32%;">
                <img src="../images/covariance/data_images/two_variances.png" alt="Discontinuity in covariance" class="small-image">
                <figcaption>Discontinuity in covariance</figcaption>
            </figure>
            <figure style="flex: 0 0 32%;">
                <img src="../images/covariance/data_images/continous_varying_variances.png" alt="Smoothly varying covariance (linear)" class="small-image">
                <figcaption>Smoothly varying covariance (linear)</figcaption>
            </figure>
            <figure style="flex: 0 0 32%;">
                <img src="../images/covariance/data_images/continous_varying_variance_sine.png" alt="Smoothly varying covariance (nonlinear)" class="small-image">
                <figcaption>Smoothly varying covariance (nonlinear)</figcaption>
            </figure>
        </div>
    </figure>
</div>

<p>Figure 5a displays a discontinuous variation in covariance. While this can still be stitched together by a fiber bundle, it is not a smooth, differentiable manifold. Discontinuous cases can be considered approximately continuous with some loss of accuracy, and we will ignore discontinuous cases in the following. Figures 5b and 5c display distributions with covariance that varies in linear and non-linear manners, respectively. In both cases, the mean follows a linear parametrization.</p>

<h3>Complex Covariance and Mean Structures</h3>
<p>More complex cases exist when both the mean \(\mu(x,y)\) and covariance \(\Sigma(x,y)\) are nonlinear. Examples are displayed in Figures below:</p>

<div>
    <figure>
        <figcaption>Figure 6: Non-linearly Varying Covariance on a Circle</figcaption>
        <div style="display: flex; justify-content: space-around;">
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/circle_mean_sine_variance.png" alt="Nonlinear distribution on a circle" class="small-image">
                <figcaption>Nonlinear distribution on a circle</figcaption>
            </figure>
            <figure style="flex: 0 0 45%;">
                <img src="../images/covariance/data_images/parabola_mean_sine_variance.png" alt="Nonlinear distribution on a parabola" class="small-image">
                <figcaption>Nonlinear distribution on a parabola</figcaption>
            </figure>
            <figure>
                <img src="../images/covariance/data_images/Gaussian_Process_Regression_data.png" alt="Nonlinear covariance distribution with sine-wave mean" class="small-image">
                <figcaption>Nonlinear covariance distribution with sine-wave mean</figcaption>
            </figure>
        </div>
    </figure> 
</div>

<p>These distributions cannot easily be modeled by a simple covariance matrix and a mean. However, locally (if the transitions are continuous and the neighborhood is small enough), they are well behaved and adhere to the linear description. If the distribution is regular in all points, then smooth transitions from one neighborhood to another can be defined by considering a Riemannian geometry on which the data-distribution lives.</p>

<p>Manifold learning exploits this local linearity by determining data-samples that are in linear neighborhoods of each other. Then, the data-distribution of these samples can be assumed to be linear. It is not always trivial to determine which points are close to each other by only looking at the dimensional data representation (e.g., pixel values).</p>
<h3>Covariance Matrix as a Geometry of Linear Space</h3>
<p>These examples serve as visualizations to help build an intuition about how a covariance matrix induces a geometry in linear space. Thus, given a covariance matrix, we might think about a stretched/compressed Euclidean space. We have examined how this can be seen as a transformation of Gaussian noise and explored more complex cases, where the data-distribution depends on the parameter. These complex cases, when continuous, can be seen as smooth transitions between linear cases. For singular distributions, problems arise due to the existence of degenerate directions. A singular covariance matrix is related to perfect correlation.</p>








<h3>Fisher Information Matrix</h3>
<p>An interesting covariance is the Fisher Information Matrix. In machine learning, one aims to maximize a likelihood \( p(x, \theta) \) or log-likelihood \( \log(p(x, \theta)) \) by finding the optimal parameters \( \theta \).</p>

<p>The gradient of the likelihood with respect to the parameters is defined as:</p>
<div class="equation-block">
    $$
s(x, \theta) = \frac{\partial \log (p(x, \theta))}{\partial \theta}
$$
</div>

<p>and is called the score of the model. It can be shown that, under regularity conditions, the score of the true model \( \tilde{\theta} \) is zero. In general, the score is indicative of the model-parameter sensitivity. The notion of a score-based gradient has been exploited successfully in generative modeling, under the name of score matching.</p>

<p>For each \( \theta \), it is possible to define the Fisher Information Matrix as the covariance of the score:</p>
<div class="equation-block">
    $$
\mathbf{I}(\theta) = \mathbb{E}\left[ \left( \frac{\partial \log p(x, \theta)}{\partial \theta} \right) \left( \frac{\partial \log p(x, \theta)}{\partial \theta} \right)^T \right]
$$
</div>

<p>which is also expressed as:</p>
<div class="equation-block">
    $$
= -\mathbb{E}\left[ \frac{\partial^2 \log p(x|\theta)}{\partial \theta \partial \theta^T} \right]
= \int_{x} \frac{\partial}{\partial \theta} \log(p(x, \theta)^2) p(x, \theta) \, dx.
$$
</div>

<p>Given the previous examples, this should create some intuition of the underlying geometry defined by this matrix. Note that this covariance matrix varies smoothly, depending on the location. Considering the distance induced by a covariance matrix, it can be shown that \( \mathbf{I}(\theta) \) is a Riemannian metric, providing the tangent space of \( \theta \) with an inner product and defining distances between distributions. The geodesic distance is the path-integral of distances induced in each point by the local covariance matrix. The idea is similar to the Mahalanobis distance, but, due to the interpretation in terms of model parameter sensitivity (as opposed to data sensitivity), we do not use the inverse.</p>

<h3>Geodesic Distance in Parameter Space</h3>
<p>Consider an infinitesimal displacement \( \frac{d\theta }{dt} = \lim_{\Delta \rightarrow 0} = \theta + \Delta \) and a path \( \gamma(t) \) related to the displacement by:</p>
<div class="equation-block">
    $$
d \theta = \frac{d \gamma(t)}{dt} \, dt.
$$
</div>


<p>The infinitesimal distance from \( \theta \) to \( d\theta \) is:</p>
<div class="equation-block">
    $$
ds^2 = d\theta^T I(\theta) d\theta = \frac{d \gamma(t)}{dt} I(\theta) \frac{d \gamma(t)}{dt} \, dt.
$$
</div>

<p>This is the infinitesimal squared distance in Riemannian geometry with the Fisher information matrix as the metric tensor. When \( I(\theta) \) is large, indicating a high likelihood sensitivity of a parameter, the distance in that direction is also large. Moving in directions with high sensitivity (i.e., corresponding to high information content) traverses a larger distance, while moving in directions with low sensitivity does not get us far. If we want to minimize the likelihood, we prefer to move the shortest path from \( \theta_1 \) to \( \theta_2 \), i.e., update the most informative parameters.</p>

<p>The distance between \( \theta_1 \) and \( \theta_2 \) becomes:</p>
<div class="equation-block">
    $$
d_G(\theta_1, \theta_2) = \int_0^1 \sqrt{\frac{d\gamma(t)}{dt}^T I(\gamma(t)) \frac{d\gamma(t)}{dt}} \, dt = \int_0^1 ds,
$$
</div>

<p>where \( \gamma(0) = \theta_1 \) and \( \gamma(1) = \theta_2 \).</p>

<p>This geodesic distance allows for the quantification of distribution similarity. Distributions (parameterized by \( \theta \)) are close if the information content is similar and distant otherwise. The geodesic distance is symmetric. For infinite step sizes it coincides with the KL-divergence</p>









<h3>Geometries of a neural net</h3>
The geometry of neural networks, auch as CNNs and Bolzman machines is focus of recent research.
<a href="https://arxiv.org/abs/2410.12025">Geometric Inductive Biases of Deep Networks</a> introduces quantities 
which study the relation between input-space covariance and model covariance.

They introduce the quantities
<div class="equation-block">
    $$
    G_t(x) = \mathbb{E}_{\theta \sim T_t} \left[ \nabla_x f_\theta(x) \nabla_x f_\theta(x)^\top \right]
$$
</div>
and
<div class="equation-block">
    $$
    \Delta t \, F(x) = \mathbb{E}_{\theta \sim T_t} \left[ \nabla^2_{x,\theta} f_\theta(x) \, \dot{\theta} \,
     \nabla_x f_\theta(x)^\top \right] + \mathbb{E}_{\theta \sim T_t} \left[ \nabla^2_{x,\theta} f_\theta(x) \, 
     \dot{\theta} \, \nabla_x f_\theta(x)^\top \right]^\top
$$
</div>
called average geometry and average geometry evolution respectively. The quantity \( T_t \) represents the distribution of 
the trajectoreis of   \( \theta \) during training. It is subject to stoachstisticity by different initalizations, mini-batching
and randomness in the training procedure.
The data-covariance is defined as the linear covariance <div class="equation-block">
    $$
   s = \sum_{i=1}^{N}x_ix_i^T = XX^T
$$
</div>

It is shown that  \(  \Delta t \, F(x)  \rightarrow G_0(x)SG_0(x) \) as  \( n \rightarrow \infty \) where \(n \) corresponds to model width. In essence,
the geometry induced by the covariance is mapped via the average geometry of the model at initalization 
(similar to a rotation and rescaling of a distribution explained above).
For multiplayer perceptrons  \(G_t(x) = I \). The same does not hold for convolutional networks.
If  \(G(x) \) is structured (i.e. has singularities), then some directions (those with eigenvalue zero) are uninformative and the model is invariant to these.
For a convolutional neural net these directions might correspond to translations of the data since a convolutional classifier 
should be invariant to translation. However, a convolutional object detector should be equivariant, not invariant, not translations.
Models fails to generalize, when decision boundary normals (  \(  \mathbb{E}_{\theta} \Delta_x f_\theta(x) \) ) are aligned with invariant directions.
The resuls show that the eigenspace generated by the average geometry of the model determines which features can be learned.
Feature-directions which are not in the eigenspace are not learned. 
<br><br>
Experiments show a correlation of linear data covariance \(S \) and transformed covariance \(GSG\).
These results support the conjecture that the average geometry of the model at initialization \(G_t(x)\) converges to \(GSG\) for small \(t \) .
Data-samples \(x \) with high correlation to the initial average geometry seem to be more impactful on model performance.
Feature directions with low correlation to the inital geometry seem to have less impact on model performance.
<br><br>
This line of research leads towards an understanding of geometrical quantities in neural networks which formalize well-known properties 
(such as invariance and equivariance) by eigenspace analysis of the average model geometry.




<h3>PCA</h3>

PCA is a popular and well understood algorithm which allows for data-distribution modelling and dimensionality reduction by considering the eigenspace 
of the covariance matrix. As such, it is a good introduction into the covariance matrix, its meaning and possible manipulations. 

Principal Component Analysis is a popular linear algorithm for systems of the form 
<div class="equation-block" id="pca linear eq">
$$
    x = f(z) = Wz
$$
</div>
or, in the probabilistic PCA setting
<div class="equation-block" id = "prob pca linear sys">
$$
    x = f(z) + \epsilon = Wz + \sigma^2\epsilon,
$$
</div>
    where $\epsilon \sim \mathcal{N}(0, I)$ and $z \in \mathcal{N}(0,I)$ so that 

<div class="equation-block" id = "prob pca linear sys">
$$
    p(x|z) = \mathcal{N}(Wz,WW^T) \text{ and } p(x)=\mathcal{N}(Wz,WW^T)
$$ 
</div>
in the former case and 
<div class="equation-block" id = "prob pca linear sys">
$$
p(x|z)  = \mathcal{N}(Wz,WW^T) \text{ and } p(x)  = \mathcal{N}(Wz,WW^T + \sigma^2 I)
$$ 
</div>

Consider white Gaussian noise  \( z \sim \mathcal{N}(0,I) \) and a mean-centered data-distribution \(p(x)  = \mathcal{N}(0,\Sigma)\) . Let \(x,z \in \mathcal{R}^{n}\) .
Given the data-set matrix \(X = (x_1,....,x_N) \in \mathcal{R}^{N \times n}\) , we can construct the singular value decomposition
<div class="equation-block" id = "pca svd">
$$
X = U \Delta V^T,
$$ 
</div>
with \(U \in \mathcal{R}^{N} \),  \( \Delta  \in \mathcal{R}^{N \times N} \) and  \(V \in \mathcal{R}^{n} \) (in the complex case we need to consider the conjugate transpose). 
We can construct the linear covariance matrices

<div class="equation-block" id = "pca svd">
$$
     XX^T = V\Delta^2V^T =  VDV^T  \in \mathcal{R}^{n\times n}
$$
</div>   
and 
<div class="equation-block" id = "pca svd">
$$
    X^TX = U\Delta^2U^T = UDU^T  \in \mathcal{R}^{N\times N}.
$$
</div>
In the real case \( U \) and \( V \) are guaranteed to be real orthogonal matrices so that \( \text{rank}(X^TX ) = \text{rank}(XX^T) \).
Both covariance matrices have different dimensionality but the same non-zero eigenvalues and equal rank. 
This equivalent nature of the two covariance matrices and their corresponding distributions has deep consequences in machine learning (especially for
Gaussian Processes) and is referred to as the data-space and feature-space duality.
Consider the question, which linear transformation of \( z \sim \mathcal{N}(0,I) \) yields the distribution  \( p(x)  = \mathcal{N}(0,XX^T) \). 
The answer is
<div class="equation-block" id = "pca solution">
$$
x = Wz = U D^{1/2}z
$$
</div> 
as becomes obvious from <a href="pca svd ">equation</a>.
Note that $U$ is the matrix of eigenvectors of the linear covariance matrix \( XX^T \) and that \( D^{1/2} \) is the square-root of its eigenvalue matrix.
Thus the linear transformation relating the data-distribution to white noise is defined by eigen-directions and eigenvalues of the covariance matrix. 
The eigenvalues are called the principal components, the eigenvectors are called principal directions. 
<br><br>
The main application of PCA is in dimensionality reduction. 
Until now a dimensionality preserving linear map was derived. 
For dimensionality reduction consider a system as in equation \ref{eq: prob pca linear sys} with 
\( x \in \mathbb{R}^n \) , 
\( z \in \mathbb{R}^d \) and consequently
\( W \in \mathbb{R}^{n \times d} \) with \( d \leq n \). 
  
Consider the distribution \( \mathcal{N}(0, WW^T) \) and note that \( WW^T \) has rank \( d \) but dimension \( n \). 
Such cases are called singular distributions (some eigenvalues are zero). Singularity indicates the existence of directions in which no variability occurs.   
Given a dataset \( X \), we can perform the PCA decomposition and yield equation \ref{eq: pca solution}. 
By keeping only the \( d \) biggest eigenvectors ( \( D_{-} \in \mathcal{R}^{d \times d} \) ) and corresponding eigenvalues (\( U_{-} \in \mathcal{R}^{n \times d}\) ) , a dimensionality reduction is performed. Note, that then 
<div class="equation-block" id = "pca solution">
$$W =  U_{-}D_{-}^{1/2}$$
</div>
<div class="equation-block" id = "pca solution">
$$ \Sigma = WW^T = U_{-}D_{-}U^T  \in \mathbb{R}^{n \times n}$$ 
 </div>

is a singular matrix with rank \(d\).
Along the directions corresponding to the omitted eigenvectors no variability occurs. 
Since singular distributions can be hard to work with, a common practice is adding noise. In essence, we consider
<div class="equation-block" id = "pca solution">
$$
x = Wz + \epsilon
$$ 
</div>
with \( W \in \mathbb{R}^{n\times d} \), \( z \in \mathbb{R}^{d} \) and  \( \epsilon \sim \mathcal{N}(0, \sigma^2I)\). 
From this equation, which corresponds to the probabilistic PCA setting (see equation \ref{eq: prob pca linear sys}), we obtain
<div class="equation-block" id = "pca solution">
$$
p(x|z) \sim \mathcal{N}(Wz, WW^T)
$$  
</div>
leading to 
<div class="equation-block" id = "pca solution">
$$
p(x)  \sim \mathcal{N}(Wz, \Sigma)
$$ 
</div>
with with  \( \Sigma = WW^T +  \sigma^2 I \approx XX^T\)
.Consider the mean centered version 
\(
p(x) \sim \mathcal{N}(0, \Sigma) 
\)
 and perform MLE,
<div class="equation-block" id = "pca solution">
$$
\Sigma_{opt} = \operatorname{argmax}_{\Sigma}  \prod_{i=1}^{N} \log( \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp  \frac{- x_i \Sigma^{-1} x_i^T}{2} )
$$
</div>
<div class="equation-block" id = "pca solution">
$$
= \operatorname{argmin}_{\Sigma}  \sum{i=1}^{N}  \log( \frac{1}{\sqrt{\((2\pi)^n |\Sigma|}}) +  \frac{- x_i \Sigma^{-1} x_i^T}{2} 
$$
</div>
<div class="equation-block" id = "pca solution">
$$
= \operatorname{argmin}_{\Sigma} -\frac{N}{2} \log((2\pi)^n) - \frac{N}{2}\log(|\Sigma|) - \frac{1}{2} \sum_{i=1}^N x_i \Sigma^{-1} x_i^T
$$
</div>
<div class="equation-block" id = "pca solution">
$$
= \operatorname{argmin}_{\Sigma}- \frac{N}{2}\log(|\Sigma|) - \frac{1}{2} \sum_{i=1}^N x_i \Sigma^{-1} x_i^T
$$
</div>
<div class="equation-block" id = "MLE mutlivar covar">
$$
= \operatorname{argmin}_{\Sigma} -\frac{N}{2}\log(|\Sigma|) -\frac{1}{2} \sum_{i=1}^N x_i \Sigma^{-1} x_i^T.
$$
</div>
  
Consider
<div class="equation-block" id = "MLE mutlivar covar"></div>
$$
\frac{\partial}{\partial \Sigma} -\frac{N}{2}\log(|\Sigma|) = -\frac{N}{2}\Sigma^{-1}
$$
</div>
and
<div class="equation-block" id = "MLE mutlivar covar"></div>
$$
\frac{\partial}{\partial \Sigma} - \frac{1}{2} \sum_{i=1}^N x_i\Sigma^{-1} x_i^T  =  + \frac{1}{2} \Sigma^{-1} \sum_{i=1}^N x_ix_i^T \Sigma^{-1} .
$$
</div>
so that the extremum is reached if 
<div class="equation-block" id = "MLE mutlivar covar"></div>
$$
\frac{N}{2} \Sigma^{-1} + \frac{1}{2} \Sigma^{-1} \sum_{i=1}^N x_i^Tx_i  \Sigma^{-1}= 0.
$$
</div>
Consider \( \sum_{i=1}^N x_ix_i^T = XX^T =  UDU^T \), according to the singular value decomposition (see equation \ref{eq: pca svd}) so that
<div class="equation-block" id = "MLE mutlivar covar">
$$
\frac{N}{2} \Sigma^{-1} + \frac{N}{2} \Sigma^{-1} UDU^T \Sigma^{-1} = 0.
$$
</div>
and thus
<div class="equation-block" id = "MLE mutlivar covar">
$$
\frac{N}{2} + \frac{N}{2} \Sigma^{-1} UDU^T = 0 \rightarrow  UDU^T = \Sigma
$$
</div>
We only wan to keep the $d$ biggest eigenvalues and eigen-directions so that
<div class="equation-block" id = "MLE mutlivar covar">
$$
WW^T + \sigma^2 I \approx  \Sigma = UDU^T  \approx U_{-}D_{-}U_{-}^T + \sigma^2 I.
$$
</div>
$$
Let, $W = U_{-}(D_{-} - \sigma^2 I)^{1/2}$ so that
<div class="equation-block" id = "MLE mutlivar covar"></div>
$$
WW^T  =  U_{-}D_{-}U_{-}^T - \sigma^2I \approx \Sigma.
$$
</div>
$D_{-}$ should contain the $d$ biggest eigenvalues and $\sigma =  \frac{1}{n-d}\sum_{j=d+1}^{n} \lambda_j$.
Thus, the dimensionality reduction results in
<div class="equation-block" id = "MLE mutlivar covar">
$$
x = U_{-}(D_{-} - \sigma^2 I)^{1/2}z + \sigma^2 \epsilon.
$$
</div>
If we omit  \( \epsilon \), this corresponds to a linear projection of the \( n \) dimensional data \( X \) onto a subspace of $d$ dimension.
<br><br>
We have shown how PCA can be considered a linear transformation of white noise to a Gaussian distribution. 
Thus given a data-distribution we can find the transformation  \(  W \) which produces our data given white noise as an input.
 We have also extended this point of view to a dimensionality reduction setting. 
 Any linear transformation of \( d \)-dimensional white noise creates a \( d\)  dimensional subspace in the \( n \)-dimensional feature space. 
 Considering noise contributions in its orthogonal directions solves the this issue and defines a regular distribution in the whole space. 
 We have shown how solving for the optimal transformation and noise parameters traces back to the eigen decomposition of the linear covariance matrix defined by \( X \). 
 Essentially, one keeps the \( d \) biggest directions and eigenvalues 
 (which have the biggest contribution to the covariance matrix) and uses the others to construct the noise parameters.
For this we have performed maximum likelihood estimation with a parameterized \( \Sigma \) matrix.
<br><br>
We have also hinted at an equivalence between the matrices \( XX^T \)and \( X^TX \). While living in different spaces, the image of both matrices has equal dimensions. 
In some sense, both distributions are equivalent.
<br><br>
PCA is a very well understood and popular method. Being completely linear allows for closed form solutions, good interpretability and analysis. 
Linearity is at the same time its strength and main limitation. As a linear transformation of white-noise we can not expect to create a model for nonlinear distributions. 
It has been shown \markred{paper} that the variational auto-encoder with a diagonally parameterized covariance matrix pursues the principal directions of PCA.
Given these results, we might consider PCA as the linear alternative of VAEs and VAEs as its nonlinear counterpart. If our data-distribution is indeed linear, both yield the same results. If our data-distribution is non-linear, PCA will fail to capture it while the VAE still might. 

<h3>Gaussian Processes and function space</h3>

Gaussian Processes parameterize a covariance matrix via a kernel-covariance. Starting from PCA they become much easier to understand,
since they can be viewed as a form of PCA in potentially infinite feature-dimensions, 
created by applying a nonlinear mapping (kernel) to the feature-space.
This mapping is implicit in the kernel-covariance parameterization and does not need to be explicitly defined. 
This becomes clearer by relation to the reproducting kernel Hilbert space (RKHS), where the basis functions explicitely define the kernel as their inner product.
Since a function is informally thought of as infinite dimensional vector an infinite-dimensional feature-space is also called function space,. 
I like to think about euclidean space with directions representing functions and non-linear relationships.  
In this euclidean space, the covariance matrix defines a linear distribution and induces a Mahanalobis distance, however, due to the possibility of 
nonlinear basis directions, the expressivity is augmented.
Since a covariance matrix in infinite-dimensional feature-spaces is not computable, gaussian processes exploit the data-space and feature-space duality 
and perform the covariance parameterization in data-space. The parameters are updated via maximum likelihood estimation.
Given an optimal covariance matrix, a gaussian distribution is defined and, by its conditional mean its conditional covariance, inference can be done with 
uncertainty information.
Note, that the model
<div class="equation-block" id = "MLE mutlivar covar">
$$
x = Wz + \epsilon
$$
</div>
leads to 
<div class="equation-block" id = "MLE mutlivar covar">
$$p(x) = \int p(x|z)p(z) dz$$
</div>
since \( z \)is white noise. If \( p(z) \) is Gaussian, then so are \( p(x|z) \) and \( p(x) \). Remember that \( XX^T \) and \( X^TX \) define equivalent distributions in different dimensions.
Consider now an alternative model.

<div class="equation-block" id = "pca to gp">
    $$
    x = W\Phi(z) + \epsilon,
    $$
</div>
where \( \Phi(z) \) is a kernel (some map into lower or higher, potentially infinite dimensions), \( z \) deterministic and \( w \sim \mathcal{N}(0,I) \) . This leads to
<div class="equation-block" >
$$p(x) = \int p(x|w)p(w) dw.$$ 
</div>
If $p(w)$ is Gaussian, then so are $p(x|w)$ and $p(x)$. 
Assume one is interested in 
<div class="equation-block" ></div>
$$p(x) \sim \mathcal{N}(W\Phi(z), \Sigma), $$
where 
<div class="equation-block" >
$$\Sigma = WW^T + \sigma^2 I$$ 
</div>
and has available a set of \( N\) data-samples \( (x_i,z_i)\).  This direct consideration of \( p(x) \) as a Gaussian distribution without concretizing \( \Phi(x)\) or \( W\) is what is referred to as an 
integration over the space of all functions (informally: infinite dimensional vectors are functions). 
However, the covariance matrix still needs to be parametrized, which limits this space of all functions to more concrete, but still very broad types.
In PCA, the prior was over \( z\) and we would just consider \( \Sigma \approx XX^T \in \mathcal{R}^{n \times n}\). We can do this because
<div class="equation-block" >
$$
\text{Var}(x) = \sum_{i=1}^N \mathbb{E}(Wz_i - \mu)\mathbb{E}(Wz_i - \mu)^T = \sum_{i=1}^N \mathbb{E}(Wz_i)\mathbb{E}(Wz_i )^T  =  W \sum_{i=1}^N\mathbb{E}(z_i)\mathbb{E}(z_i)^T W^T  =  W W^T. 
$$
</div>
However, for equation \ref{eq:pca to gp} we can not do this since the map \( \Phi(z) \) is not known (not even its dimensionality). 
We have
<div class="equation-block" >
$$
\text{Var}(x) = \sum_{i=1}^N\mathbb{E}( W\Phi(z_i))\mathbb{E}(W\Phi(z_i))^T  =  \sum_{i=1}^N \Phi(z_i)\mathbb{E}(W)\mathbb{E}(W)^T\Phi(z_i)^T 
$$
</div>
A way of alleviating the issue of an unknown covariance matrix with unspecified feature dimensions, 
is by considering the covariance \( \Sigma = X^TX \in \mathcal{R}^{N \times N} \) which is limited in dimensionality only by the amount of data-samples, 
but defines a distribution in the feature-space dimensions, as illustrated on the linear case. 
One chooses a non-linear parameterization of the kernel covariance matrix \( \Sigma = K(Z,Z) \), such as the radial basis function kernel. 
This imposes a form on the corresponding functions \( \Phi(z) \). The maximum likelihood estimation from equation \ref{eq: MLE mutivariate covar} 
with respect to the kernel parameters can be performed for non-linearly parameterized matrices. For some kernel covariances it can the solved analytically. 
This is the concept behind Gaussian processes.
Given the covariance matrix which maximizes our likelihood 
<div class="equation-block" >
$$p(x) = \int p(x|w)p(w) dw = \mathcal{N}(0, K),$$
</div>
we can consider
<div class="equation-block" >
$$
    \begin{bmatrix}
        p(x) \\
        p(x^*)
    \end{bmatrix}  = \mathcal{N}(\begin{bmatrix} \mu_X, \mu_x^*\end{bmatrix}  ,\begin{bmatrix} K, K(Z,z^*) \\ K(z^*,Z), k(z^*,z^*)\end{bmatrix}).
$$
</div>
The conditional mean and variance of a multi-variate Gaussian is
<div class="equation-block" ></div>
$$ \label{eq:conditional mean}
\mathbb{E}[x^* | X ] = \mu_X + K(z^*,Z)  K^{-1}(X)
$$
</div>
and
<div class="equation-block" >
$$ \label{eq:conditional variance}
\text{Var}(x^* | X) = k(z^*,z^*) - K(z^*,Z) K^{-1} K(Z,z^*).
$$
</div>
These quantities allow for inference with associated uncertainty information. 
Usually we consider mean centered data, so that \( \mu_X = 0 \). 
Note, that in this framework, we assume an available data-set of pairs \( x_i,z_i \) and \( z_i \) is not associated to a prior distribution.
<br><br>
In an alternative setting, the data consists only of \(x_i \) and the maximum likelihood is optimized also with respect to \(z_i \). 
These types of models are called Gaussian process latent variable models (GPLVM). 
The dimension of \( z \) is a hyperparameter, making this model suitable for dimensionality reduction. 
Similar to the auto-encoder, the latent-space is unrestricted, so that smoothness and continuity of the latent distribution is not necessarily given. 
 Similar to the variational auto-encoder, a natural extension to bayesian variational gaussian process latent variable models exists.
However, within the community, Gaussian Processes are used mainly for regression tasks. 

<br><br>
Interesting extensions of gaussian processes are gaussian process latent variable models (GPLVM) and variational GPLVM.
GPLVM is a generative model which allows for dimensionality reduction and data generation, similar to an auto-encoder.
Variational GPLVM imposes a variational prior onto the latent variables, similar to a variational auto-encoder.
<br><br>
Some approaches connect gaussian processes with auto-encoders by 








<h3>Attention</h3>

The attention mechanism, popularized by the transformer model, can be understood as a learned covariance parameterization.
The mapping of pairs to a scalar product (to a reproducing hilbert space) corresponds to learning the covariance.
This scalar product can also be understood as inducing a geometry in a reproducing hilbert space 
(infinite dimensional vector space),
with zero corresponding to orthogonality and one corresponding to perfect correlation. 
Note, that the covariance matrix stretches and compresses a linear space allowing for learning a distortion. 
Attention moves close points closer together (higher covariance) and far points further away (lower covariance).
Compare this intuition to the Mahanalobis distance, which is a distance measure in a stretched and compressed space.
<br>
<br>
To explain the attention mechanism start with a sequence of embeddings, or vectors, \( \{e_1, ..., e_n \} \), each with dimension \(d\). 
Assume each vector also contains information on its position \( i \in \{ 1, ... , n \} \).
An attention block aims to compute an updated version of the embedding so that each contains information of its context. 
Each input-embedding attends to all other input-embedding. This enables the encoding of global context in the single embedding.
Global context is just a learned aggregation on the whole data. 
This contrast many other methods, where the aggregation is restricted to neighbourhoods, such as convolutional filters and RNN's.
Considering the whole data makes attention powerful but computationally expensive. 
Because in an attention module all embeddings attend to all others, the context window is global instead of sequential.  
Thus, the embedding for pronouns can learn to pay attention to the embedding for the sentence subject, while the proposition "to" learns to pay attention 
to the connected verb "walking" and noun "school". 
In addition, positional encodings are used, providing information on the embedding location. 
In some languages the noun is always the first word which can be recognized with positional information. 
As an example, if the vectors embedding correspond to words, the noun embedding should contain the information of the corresponding adjective. 
Each embedding vector has a query vector, usually of smaller dimension. It is computed by \( W_Q e_i = q_i \).
The query vector encodes the context. For example the notion that nouns have preceding adjectives. 
I like imagining that the query matrix asks "what are you?" to the embedding, and the embedding for orange answers "A juicy fruit" or "a color", 
depending on its embedding vector (which contains the context that is encoded).
Additionally a key is computed \( W_k e_i = k_i \) having equal dimension to the query vector. 
Given an embedding with a specific query vector, the keys which are close to that query should come from embeddings which are conceptually close.
In essence, when the \textit{dot product} between \( q_i \) and \( k_j \) is large, then \( i \) and \( j \) are conceptually close. 
For example, the query of the sky embedding could be close to the key of the blue (but not green) embedding. 
The embedding of \textit{blue} attends to the embedding of sky. 
The query vector of the orange fruit embedding might be close to the key vector of juice, tree, banana, breakfast. 
It might be far from petroleum, graphene, lithium.  
The dot product between query and key vector corresponds to a weight which assigns to each embedding an attention value reflecting how important 
they are to each other. The \textit{dot product} is passed through a \textit{softmax} layer to create a value range from zero to one.
This is the attention value. 
<br>
<br>
Let \( Q =  \{q_i \}= \{ q_1, ... ,q_n \} \) and \( K =  \{k_j \} = \{ k_1, ... ,k_n \} \) be matrices corresponding to the key and query vectors for the 
\( n \) embeddings.
Then
<div class="equation-block">
    $$
    \text{softmax}( \frac{QK^T}{\sqrt{n}} )
    $$
</div>
corresponds to the attention values computed column by column creating an attention value matrix. 
Each entry, denoted by  \( a_{ij} \), reflects the importance of embedding  \( i \) to embedding  \( j \). 
The division with  \( \sqrt{n} \) happens for numerical reasons. Notice that this matrix has dimension  \( n \times n \).
This matrix is then used to update the  \( d \) dimensional embedding. 
This notion of mapping vectors into a inner-product space is similar to kernel methods and reproducing (kernel) hilbert spaces. 
Usually these methods use parameterized filters which define the inner-product, such as the gaussian kernel. 
In the attention mechanism, these kernels are learned via the query and value matrix respectively. 
<a href="https://tanmnguyen89.github.io/gp_transformer.pdf">Revisiting Kernel Attention with Correlated Gaussian Process Representation </a> has extended this concept to Gaussian Processes. 
\newline
\newline
Until now, we have explained how to determine contextual closeness via the Query and Key matrix but not how to update embeddings according to this contextual 
closeness.
For this consider the value-vector computed by  \( W_v e_i = v_i \), where  \(v_i \) is also  \(d \)-dimensional.
This results in  \(n \) value-vectors. 
These value vectors are added to the original embeddings  \(e_i \) to do a contextual the update. For each query  \(q_i \) compute 

<div class="equation-block">
    $$u_i = \sum_{j=0}^{n}a_{ij}*v_i.$$ 
</div>
This essentially is an attention weighted sum of all value vectors  \( v_i \).
Then 
<div class="equation-block">
    $$ e_i^{t+1} = e_i^{t} + u_i $$ 
</div>
corresponds to the update of the embedding.
This process of updating the embeddings  \( e_i \) with the key \( W_k \), query  \( W_q \) and value  \( W_v \) matrices is one head of attention.
 The three matrices are the tuneable model parameters related to this computation.
Often the notation is
<div class="equation-block">
    $$
    \text{Attention}(Q,K,V) = \text{softmax}( \frac{QK^T}{\sqrt{n}})V
    $$
</div>
Using multiple key, query and value matrices is called a \textit{multi attention-head}. 
The computation of parameters for each attention head can be done in parallel. 
During the embedding update, all attention heads are used to compute the weighted change of the embedding
<div class="equation-block">
    $$
    u_i^{k} = \sum_{j=0}^{n}a_{ij}^{k}*v_i^{k},
    $$
</div>
which is then added to the embedding accordingly
<div class="equation-block">
    $$ e_i^{t+1} = \sum_{k} e_i^{t} + u_i^{k}.$$
</div>
This operation is the main ingredient in Transformer architectures. 
While it has been popularized mainly through applications in the language processing domain, 
many other areas can be included. 
<a href = "https://www.ecosia.org/search?q=Vision+Transformer&addon=chrome&addonversion=6.0.4&method=topbar">Vision Transformers</a> use attention in a visual context by splitting the Image into patches, 
and encoding these into a sequence of embeddings. Once the information is available in this for multi-head attention can be applied as described above.










<h3>Reproducing Hilbert Space</h3>

The Reproducing Hilbert Space is a vector space of functions (infinite dimensional vectors) induced with an inner product. 
This inner product allows for the notion of orthogonality and correlation. In a reproducing hilbert space it has a reproducing kernel property
<div class="equation-block">
    $$ 
    f(z) = \sum_{n=0}^{N} a_n k(x_n,z)  = \sum_{n=0}^{N} a_n k_{x_n}(z)
    $$.
</div>
By the representer theorem, every function that minimizes an empirical risk functional 
can be represented as a linear combination of the inner products evaluated at the training points.
Note the similarity to Kernel-covariances, which can thus be understood as mapping data-sample pairs into this space by defining their inner product.
Similariy, the attention mechanism can be understood as learning a kernel function by computing an inner product between query and key. 
However, for a transformer, the  \( \text{softmax}( \frac{QK^T}{\sqrt{n}} ) \) can be asymmetric and thus not ammenable to a reproducting kernel hilbert space. 
Workarounds exist which allow for asymmetric attention but enable symmetric kernel definitions enabling inference with uncertainty information as in gaussian processes.
<br><br>
The formalization of a reproducing hilbert space can be done by
considering the space of functions which are linear combinations of /( N = \infty /) orthonormal bases (basis functions) 
<div class="equation-block">
    $$ 
        f(x) = \sum_{n=1}^{N} a_n l_n(x).
    $$
</div>
Let us assume that  /( \sum |a_n|^2 dx \leq 1 /) and  /( -1 \leq a \leq 1/).
Consider 
<div class="equation-block">
    $$ 
    k(x,x) = \langle f(x)|g(x) \rangle = \langle \sum_{n=1}^{N} a_n l_n(x) | \sum_{i=1}^{m} b_n l_n(x) \rangle = \sum_{n1}^{N} a_n b_n
    $$
</div> which defines an inner product of a Hilbert space.
Let us define  \( k(z,x) =  \langle f(z)|g(x) \rangle = \sum c_n l_n(z)l_n(x) = \sum_{n=0}^N c_n z^n x^n  \) 
which converges (let us assume \(\sum c_n = 1 \), usually one requires \(a_n\) to be square integrable, so that \( \sum a_nb_n\ = \sum c_n\) converges to some value  \(c=1\) ) 
for \(zx \leq 1 \) to
<div class="equation-block">
    $$  
     \sum_{i=1}^{n} c_n z^n x^n = \frac{c}{1 - zx} 
     <n></n> 
    $$.
</div>
Here we have chosen the functions /( l_n(x) = x^n /) as basis functions.
The convergence of  \( k(x,z) = \sum l_n(z)l_n(x)  \) and of \( \sum c_n \) are an important requirements on reproducing hilbert spaces. 
The first constrains the basis functions to be square integrable, the second contrains the coefficients to be square integrable.
Note,
<div class="equation-block">
    $$ 
    \langle k(z,x) | f(x)\rangle =\langle f(x)| k(z,x) \rangle = \langle f(x)|  \frac{c}{1 - l(z)l(x)} \rangle 
  = \langle f(x)| \frac{1}{1 - zx} \rangle = \sum_{n=1}^{N} a_n k(x,z) = \sum_{n=1}^{N} a_n z^n  = \sum_{n=1}^{N} a_n l_n(z) = f(z)
    $$.
</div>
This is the reproducing property of the kernel associated to the hilbert space. Thus a hilbert space with such a kernel is called reproducing hilbert space.
<br><br>
In another example, consider <div class="equation-block">
    $$ 
    l_n(x) = \frac{x^n}{\sqrt{n!}} \exp^{\frac{-x^2}{n}} /
    $$
</div> which defines a basis of gaussian functions. As before we obtain
<div class="equation-block">
    $$ 
    k(x,x) =  \langle f(x)|g(x) \rangle = \langle \sum_{n=1}^{N} a_n l(x) | \sum_{i=1}^{n} b_n l(x) \rangle = \sum_{n=1}^{N} a_n b_n
    $$
</div>
and
<div class="equation-block">
$$  
k(z,x) =  \sum_{n=1}^{N} c_n l_n(z)l_n(x) = \sum_{n=1}^{N} c_n ( \frac{z^n}{\sqrt{n!}} \exp^{\frac{-z^2}{n}} ) (\frac{x^n}{\sqrt{n!}} \exp^{\frac{-x^2}{n}}) 
$$
$$
= c * \exp^{ \frac{ -(z-x)^2 }{ 2 } }
$$
</div> which is the gaussian kernel function. So the gaussian kernel function is the inner-product for the reproducing hilbert space of gaussian functions.
As before 
<div class="equation-block">
    $$ 
    \langle f(x) | k(z,x) \rangle =  \langle k(z,x) | f(x) \rangle = \sum_{n=0}^{N} a_n k(x_n,z)  = \sum_{n=0}^{N} a_n k_{x_n}(z) = \sum_{n=0}^{N} a_n l_n(z) = f(z).
    $$.
</div>
The typical notation for writing functions as linear combinations of reproducing kernels is 
<div class="equation-block">
    $$ 
    f(z) = \sum_{n=0}^{N} a_n k(x_n,z)  = \sum_{n=0}^{N} a_n k_{x_n}(z)
    $$.
</div>
This allows for the representation of functions in the reproducing hilbert space. 
The parameters /( a_n /) are learned by gradient methods.
Note, that the reproducing kernel is symmetric and positive definite, thus defining a manifold and allowing for an interpretation 
as a regular (or singular) elliptical distribution in the infinite dimensional (functional) Hilbert space.
Note the similarity to gaussian processes where inference is done via
<div class="equation-block" ></div>
$$ \label{eq:conditional mean}
\mathbb{E}[x^* | X ] = \mu_X + K(z^*,Z)  K^{-1}(X)
$$
</div>

<h3>Kernel Attention</h3>

There exist mechanisms which enable using attention to learn a kernel function. 
Since the kernel of a reproducing hilbert space needs to be symmetric and positive definite,
this imposes some restrictions on the attention mechanism.
<a href="https://arxiv.org/abs/2303.02444">Calibrating Transformers via Sparse Gaussian Processes</a> explors
a mechanism for learning covariances between key and query pairs, enabling inference in a repoducing hilbert space with uncertainty information.
The attention mechanism is constrained so that the resulting kernel is symmetric and positive definite.
<a href="https://tanmnguyen89.github.io/gp_transformer.pdf">Revisiting Kernel Attention with Correlated Gaussian Process Representation</a>
proposes a method which avoids these restrictions and constructs a symmetric kernel from two gaussian processes, allowing for asymmetric softmax-functions 
between query and key. This enables greater flexibility. Intuitively, asymmetry might enable representations where, 
e.g. the embedding for sky is closer to the embedding to blue than the embedding of blue to the embedding of sky. 
This asymmetric does not adhere anymore to an intuitive description (at least not to mine) as a covariance matrix. 



<h3>More thoughts</h3>

<h4>Function space latent space</h4>
Often the isotropic gaussian prior of variational autoencoders is too restrictive.
It could be interesting to use a reproducing hilbert space as the latent space of a variational autoencoder or an autoencoder in general.
Some interesting research in this direction is
<a href="https://arxiv.org/pdf/2408.01362">Autoencoders in Function Space</a>
and 
<a href="https://arxiv.org/abs/1805.11028">Autoencoding any Data through Kernel Autoencoders</a>.
The first shows a comparable performance of their architecture to convolutional neural nets. 

<h4>Kernels with inductive biases</h4>
The kernel autoencoder does not have the convolutonal inductive bias.
It would be interesting to see if kernels can be restricted to represent equivariant and invariant functions only, 
thus replicating the inductive bias of convolutional neural nets and allowing for better generalization.
For example, using the right kind of basis functions, such as spherical harmonics (which are SO3 equivariant)
might enable using equivariant functions for inference with uncertainty information. I
t is not clear to me, if the corresponding kernel can be computed in reasonable time.
Similariy, using the fourier basis might also be interesting. 
Another possibility could be the construction of kernels on basis splines.

<h4>Kernels on basis splines for learning simulations on meshes</h4>
Basis splines are interesting since they allow for a more controlled function approximation (compared to the exponential basis). 
For example, one might learn the basis function coefficients employed in numerical simulations while imposing the relevant boundary conditions. 
Using bayesian inference with kernels allows for uncertainty quantification in the function approximation, which is generally desirable, 
but especially important when learning real-world properties in-silico.

<br><br>

<h4>Kernels for learning function expansions</h4>
In quantum chemistry, the wave function is approximated as linear combination of slater-determinant, potentially also allowing for application of kernel methods.
A problem can be the high parameter count.
Generally, all functions which adher to the formulation 
<div class="equation-block">
    $$ 
        f(x) = \sum_{n=1}^{N} a_n l(x).
    $$
</div>
are amenable to a reproducing hilbert space, kernalized learning and inference with uncertainty information. 
The kernel function is then
<div class="equation-block">
    $$ 
    k(x,y) = \sum_{n=1}^{N} a_n l(x) l(y).
    $$
    $$
</div> under conditions of convergence on its fourier series with respect to the orthonormal system /( l(x) /).
It might not be trivial to obtain and compute the right kernel function. 
If the real functions has component that are orthogonal to the space of basis-functions, this part can not approximated.

<!-- 
<h2>Connections to Algorithms</h2>
<p>Let us connect the concepts mentioned to common machine learning algorithms. In all cases, we assume a data-density:</p>
<div class="equation-block">
    $$
q(x) = \int q(x, z) \, dz = \int q(x | z) q(z) \, dz
$$
</div>

<p>where \( z \) corresponds to an unobserved latent variable. In generative models, this latent variable \( z \) might "generate" the sample \( x \). The latent \( z \) can also be connected to the data-sample \( x \) without an explicit generative role, such as an unobserved label. The field of self-supervised learning aims to learn the labels by considering them as latent variables \( z \).</p>

<p>A common complication is that the integral might not be tractable. In this case, a lower bound (the ELBO) is often constructed. Model-parametric approaches use a parameterized description of the data-density of the form:</p>
<div class="equation-block">
    $$
p_{\theta, \phi}(x) = \int p_\theta(x | z) p_\phi(z) \, dz.
$$
</div>

<p>An example of a model that adheres to this type of description is the variational autoencoder. The decoder parameterizes \( p_\theta(x | z) \), and the encoder parameterizes \( p_\phi(z) \). The Gaussian Mixture Model also belongs to this class of methods, with some tractability properties for \( p_\phi(z) \).</p>

<p>Model-free approaches do not use a model-parameterization. Instead, they aim to extract useful information directly from the data. One might say that each data-sample is a parameter. Common examples include PCA and Gaussian Processes. The extracted information (such as the principal directions for PCA) can be thought of as the unobserved latent variable \( z \) giving rise to the data distribution.</p>

<p>In Gaussian Processes, the covariance structure of the data is "learned" by imposing a prior structure via kernels. These kernels introduce prior assumptions and have some hyperparameters, so they might be understood as a model-parameterization. However, the key difference is that there is no explicit description of \( p_\theta(x | z) \) or \( q_\phi(z) \). Instead, each data-sample contributes to the learned covariance matrix.</p>

<h3>Principal Component Analysis (PCA)</h3>
<p>PCA is a method that calculates the covariance matrix given a number of \( N \) data-samples, each with dimension \( n \). By diagonalizing the matrix, one obtains "directions" within the data-parameters that independently introduce variability into the data. The directions with the highest eigenvalues introduce more variability than those with the lowest. A common dimensionality reduction method expresses the data only in terms of the eigenvectors corresponding to the \( m \) highest eigenvalues, with lower-eigenvalue directions considered as noise.</p>

<p>Given the data \( X \), which can be described as a set of \( n \)-dimensional vectors \( X = \{ \vec{x_i} \} \) with \( i = 1, \dots, N \), the linear covariance matrix \( \Sigma \in \mathcal{R}^{n \times n} \) is computed:</p>
<div class="equation-block">
    $$
\vec{\mu} = \frac{1}{N} \sum_{i=1}^{N} \vec{x_i}
$$
</div>

<div class="equation-block">
    $$
\Sigma = \frac{1}{N-1} \sum_{i=1}^{N} (\vec{x_i} -  \vec{\mu} ) (\vec{x_i} -  \vec{\mu} )^{T}.
$$
</div>

<p>This matrix is symmetric and, therefore, always diagonalizable. It can be written in the form:</p>
<div class="equation-block">
    $$
\Sigma = U D U^{-1}
$$
</div>

<p>where \( U = [ \vec{u_{1}}, \dots, \vec{u_{N}} ] \) and \( D \) is a diagonal matrix of eigenvalues.</p>

<h3>Transformation to Gaussian Noise</h3>
<p>As shown in previous sections, any linear distribution can be obtained by transforming white noise. Thus, we can ask which transformation of white noise corresponds to our data. Following the logic in previous sections, the answer is:</p>
<div class="equation-block">
    $$
x = U^T D^{1/2} z + \mu = Wz + \mu,
$$
</div>

<p>where \( z \sim \mathcal{N}(0, I) \) and \( x \sim \mathcal{N}( \mu, \Sigma ) \). The mean is often omitted as it corresponds to a simple shift. In this case, \( W \in \mathcal{R}^{n \times n} \) and \( z \in \mathcal{R}^n \), meaning no dimensionality reduction is performed.</p>

<p>By keeping only the first \( m < n \) components (the \( m \) eigenvectors with the highest eigenvalues), one can describe the data in a lower-dimensional space while still modeling most of the variation. This reduction method helps focus on the principal components of the data with the highest variability, effectively reducing noise.</p>

<h3>Singular and Probabilistic PCA</h3>
<p>The linear transformation of white noise leads to:</p>
<div class="equation-block">
    $$
x = W_m z_m = U_m D_m^{1/2} z_m,
$$
</div>

<p>where \( W_m = U_m D_m^{1/2} \in \mathcal{R}^{n \times m} \) is a matrix of scaled eigenvectors for the reduced dimensions. The coordinates \( z_m \sim \mathcal{N}(0, I_m) \) in the lower \( m \)-dimensional space produce the transformed distribution.</p>

<p>Probabilistic PCA extends this model by adding noise:</p>
<div class="equation-block">
    $$
x = W z + \epsilon,
$$
</div>


<p>where \( \epsilon \sim \mathcal{N}(0, \sigma^2 I) \). Now:</p>
<div class="equation-block">
    $$
x \sim \mathcal{N}(0, WW^T + \sigma^2 I),
$$
</div>

<p>which solves the problem of degenerate directions, giving the distribution a full rank of \( n \) in feature space.</p>

<h3>Dual Space (Data-space and Feature-space)</h3>
<p>In PCA, we can construct a covariance matrix using either data or feature dimensions. The covariance matrix from the feature space:</p>
<div class="equation-block">
    $$
\Sigma = XX^T \in \mathcal{R}^{n \times n},
$$
</div>

<p>reflects the variability along the feature dimensions, while the covariance matrix from data-space:</p>
<div class="equation-block">
    $$
K = X^TX \in \mathcal{R}^{N \times N},
$$
</div>

<p>represents correlations among data points. If \( N < n \), working with \( X^TX \) is computationally advantageous, as it is only \( N \times N \).</p>
<h3>Gaussian Processes</h3>
<p>In a Gaussian Process, the covariance matrix is described via a kernel function \( k(\cdot, \cdot) \). A common choice is the radial basis function (RBF) kernel, which defines a similarity measure between points:</p>
<div class="equation-block">
    $$
k(z, z') = \sigma^2 \exp\left(-\frac{(z - z')^2}{2 l_1^2}\right),
$$
</div>

<p>This function attributes a scalar value to pairs of samples \( z \) and \( z' \), similar to an inner product. It creates a structure called a Reproducing Kernel Hilbert Space (RKHS). Here, the hyperparameters are \( \sigma \) and \( l_1 \), which influence the covariance structure, and are chosen based on prior assumptions or optimized during training.</p>

<p>Gaussian Processes (GPs) construct a covariance matrix in data-space as follows:</p>
<div class="equation-block">
    $$
\Sigma_K = \begin{bmatrix}
k(z_1, z_1) & k(z_1, z_2) & \ldots & k(z_1, z_n) \\
k(z_2, z_1) & k(z_2, z_2) & \ldots & k(z_2, z_n) \\
\vdots & \vdots & \ddots & \vdots \\
k(z_n, z_1) & k(z_n, z_2) & \ldots & k(z_n, z_n)
\end{bmatrix}.
$$
</div>

<p>This matrix is similar to the covariance matrix in dual PCA, as it considers the covariance between data samples rather than features. Gaussian Processes are used to infer the function values \( f(z) \) at unobserved points. Given observed data \( X = ((\tilde{x}_1, z_1), (\tilde{x}_2, z_2), \dots, (\tilde{x}_N, z_N)) \), the centered data \( x_i = \tilde{x}_i - \mu = f(z_i) \) can be analyzed under the GP model.</p>

<p>Using a kernel, GPs allow for the modeling of nonlinear relationships. The resulting covariance matrix \( K \) defines a geometry in data-space, with directions of compression or stretching determined by the kernel. While GPs assume an elliptical distribution in data-space, the choice of kernel can implicitly define a high-dimensional feature space with directions that capture complex patterns.</p>

<h3>Estimation in Gaussian Processes</h3>
<p>Given a new latent \( z^* \), we can relate it to existing latent variables as follows:</p>
<div class="equation-block">
    $$
\begin{pmatrix} x \\ x^* = f(z^*) \end{pmatrix} \sim \mathcal{N} \left( \mu = \begin{pmatrix} 0 \\ m(z) \end{pmatrix}, K = \begin{pmatrix} K(Z, Z) & k(Z, z^*) \\ k(z^*, Z) & k(z^*, z^*) \end{pmatrix} \right).
$$
</div>

<p>This distribution enables predictions for new data points \( x^* \) based on existing data. We aim to find the expectation \( \mathbb{E}[x^* | x] \). After some calculations, we obtain:</p>
<div class="equation-block">
    $$
\mathbb{E}[x^* | x] = k(Z, z^*)^T K(Z, Z)^{-1} x
$$
</div>

<p>and the variance:</p>
<div class="equation-block">
    $$
\mathbb{Var}[x^* | x] = k(z^*, z^*) - k(Z, z^*)^T K(Z, Z)^{-1} k(Z, z^*).
$$
</div>

<p>This gives us a method to estimate \( x^* = f(z^*) \) given observed samples \( (x_i, z_i) \). GPs model the nonlinear relationships in data-space, allowing the covariance matrix \( K \) to capture complex patterns while maintaining computational tractability.</p>

<h3>Gaussian Process Latent Variable Model (GPLVM)</h3>
<p>The Gaussian Process Latent Variable Model (GPLVM) is a variant of the Gaussian Process where the latent variable \( z \sim \mathcal{N}(0, I_m) \) is unobserved. Here, we only observe \( x_i \in \mathcal{R}^n \) and aim to learn corresponding \( z_i \in \mathcal{R}^m \) values, assuming:</p>
<div class="equation-block">
    $$
x = f(z) + \epsilon
$$
</div>


<p>where \( \epsilon \sim \mathcal{N}(0, I) \) adds regularization, helping define a full-rank distribution in feature space. Similar to probabilistic PCA, this approach extends the modeling capacity to nonlinear functions without requiring explicit parameterization, making it suitable for dimensionality reduction and capturing complex relationships in the data.</p>
<h2>Expectation Maximization</h2>
<p>Expectation Maximization (EM) is an iterative optimization algorithm often used when dealing with latent variables. In machine learning, it is particularly useful for maximum likelihood estimation in cases where the model involves unobserved data. EM iteratively optimizes a lower bound on the likelihood, called the Evidence Lower Bound (ELBO), to find parameters that maximize the observed data likelihood.</p>

<p>Consider a model with observed variables \( x \) and latent variables \( z \), where the goal is to estimate parameters \( \theta \) that maximize the likelihood \( p(x | \theta) \). The EM algorithm iteratively updates two steps:</p>

<ol>
    <li><strong>E-step:</strong> Compute the expected value of the log-likelihood with respect to the current estimate of the distribution of the latent variables, using the posterior \( p(z | x, \theta) \).</li>
    <li><strong>M-step:</strong> Maximize this expected log-likelihood with respect to \( \theta \), updating the model parameters.</li>
</ol>

<p>After repeating these steps, the algorithm converges to a local maximum of the likelihood function, providing an estimate of the parameters \( \theta \). This framework applies to a wide range of models, such as Gaussian Mixture Models (GMMs) and Variational Autoencoders (VAEs).</p>

<h3>EM in Gaussian Mixture Models (GMMs)</h3>
<p>In Gaussian Mixture Models, the data \( x \) is assumed to be generated by a mixture of multiple Gaussian distributions, each representing a cluster. The latent variables represent the assignment of each data point to a specific cluster. EM is used to estimate the parameters of each Gaussian component and the assignment probabilities.</p>

<p>During the E-step, the probability of each data point belonging to each cluster is calculated, given the current parameter estimates. In the M-step, the parameters (mean, covariance, and mixing coefficients) of each Gaussian component are updated to maximize the expected log-likelihood of the observed data. The process repeats until convergence.</p>

<h3>EM in Variational Autoencoders (VAEs)</h3>
<p>In VAEs, the latent variable \( z \) is typically a low-dimensional representation of the observed variable \( x \). The VAE aims to maximize the likelihood of \( x \) by using a recognition network to approximate the posterior \( p(z | x) \) with a simpler distribution \( q(z | x) \). The objective function, or ELBO, consists of a reconstruction term (likelihood) and a regularization term (KL divergence).</p>

<p>The VAE framework can be seen as an application of EM, where the recognition network performs the E-step, estimating the distribution over \( z \), and the generative network performs the M-step, updating the parameters of the distribution of \( x \) given \( z \). The iterative optimization allows the VAE to learn a latent space representation that captures the underlying structure of the data.</p>

<h2>Conclusion</h2>
<p>This document has covered various aspects of the geometry of the covariance matrix and its implications in machine learning, particularly in the context of linear and non-linear transformations, Gaussian Processes, and latent variable models. By exploring the Mahalanobis distance, Fisher Information Matrix, and Riemannian geometry, we’ve developed an understanding of how covariance defines a geometry in linear space, with applications in PCA, Gaussian Processes, and Expectation Maximization.</p>

<p>The dual space representation highlights the flexibility of data-space and feature-space perspectives, allowing for efficient computations and interpretations in high-dimensional spaces. In particular, Gaussian Processes extend beyond linear relationships by defining a covariance function that enables complex, nonlinear relationships to be modeled in data-space, ultimately leveraging latent representations to capture intricate data patterns.</p>

<p>With these insights, we see that covariance and distance metrics form a foundational understanding for algorithms in machine learning, allowing for both interpretability and flexibility in capturing data structures. Methods like EM offer optimization techniques that capitalize on these concepts to iteratively maximize likelihoods, making them powerful tools for models involving latent variables and for inferring complex data distributions.</p>
-->
</body>
</html>
