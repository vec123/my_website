<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Score Matching</title>

    <!-- MathJax for rendering math expressions -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>

    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
        h1, h2, h3 {
            margin-bottom: 10px;
        }
        .equation-block {
            margin: 20px 0;
            text-align: center;
        }
        p {
            line-height: 1.6;
        }
        .highlight {
            color: red;
        }
    </style>
</head>
<body>

<h1>Score Matching</h1>
<p class="author-date">vic-bayer, October 2024</p>

<h2>Introduction</h2>
<p>Score-matching is a method for likelihood maximization applied to models of the form:</p>
<div class="equation-block">
    $$p_\theta(x) = \frac{\tilde{p}_\theta(x)}{Z_\theta}$$
</div>
<p>where \( Z_\theta = \int \tilde{p}_\theta(x) \, dx \) is a generally intractable normalization constant. An example of such a model is an energy-based model, which enjoys a very flexible parameterization as a Boltzmann distribution (also called Gibbs distribution) with the form:</p>
<div class="equation-block">
    $$p_\theta(x) = \frac{e^{-f_\theta(x)}}{\int e^{-f_\theta(x)} \, dx}$$
</div>
<p>Here, \( f_\theta(x) \) denotes the energy of the distribution in analogy to the Boltzmann distribution. In the context of energy-based model optimization, \( \int e^{-f_\theta(x)} \, dx \) is often approximated using Monte Carlo methods, or more specifically, Langevin dynamics. This is generally computationally expensive, although in low dimensions, it might be amenable to parallelization via rejection sampling.</p>

<h2>Score Matching Methodology</h2>
<p>The score refers to:</p>
<div class="equation-block">
    $$\nabla_x \log p(x, \theta)$$
</div>
<p>which is different from the score \( \nabla_\theta \log p(x, \theta) \) used in statistical literature. The idea is that:</p>
<div class="equation-block">
    $$\nabla_x \log p(x, \theta) = \nabla_x \log \left( \frac{e^{p_\theta(x)}}{\int e^{p_\theta(x)} \, dx} \right) = \nabla_x p_\theta(x) - \nabla_x Z_\theta = \nabla_x p_\theta(x)$$
</div>
<p>In essence, the score function does not depend on the intractable integral while still containing information about the distribution. As the name suggests, score matching aims at matching the gradient of the empirical data density \( q(\cdot) \) with the gradient of the model density \( p(\cdot, \theta) \), leading to an optimization of the form:</p>
<div class="equation-block">
    $$\theta_{\text{opt}} = \operatorname*{argmin}_{\theta} \frac{1}{2} \mathbb{E}_{q} \left\| \nabla_x \log q(x) - \nabla_x \log p(x, \theta) \right\|_2^2$$
</div>

<h2>Further Analysis</h2>
<p>Consider the expression:</p>
<div class="equation-block">
    $$\frac{1}{2} \left\| \nabla_x \log q(x) - \nabla_x \log p(x, \theta) \right\|_2^2 = \frac{1}{2} (\nabla_x \log q(x))^2 + \frac{1}{2} (\nabla_x \nabla_x \log p(x, \theta))^2 - \frac{1}{2} \nabla_x \log q(x) \nabla_x \log p(x, \theta)$$
</div>
<p>The first term does not depend on \( \theta \), so it is not relevant for the optimization problem.</p>

<h2>Expectation of the Cross-Term</h2>
<p>Now consider:</p>
<div class="equation-block">
    $$\mathbb{E}_{q} \left[ -\nabla_x \log q(x) \nabla_x \log p(x, \theta) \right] = \int -\nabla_x \log q(x) \nabla_x \log p(x, \theta) q(x) \, dx$$
</div>
<p>This simplifies using integration by parts to:</p>
<div class="equation-block">
    $$\int -\nabla_x q(x) \nabla_x \log p(x, \theta) \, dx = -\lim_{b \to \infty} \nabla_x \log p(b, \theta) q(b) + \lim_{a \to -\infty} \nabla_x \log p(a, \theta) q(a) + \int \nabla_x^2 p(x, \theta) q(x) \, dx$$
</div>
<p>The last equality follows from integration by parts. Here, <span class="highlight">Hyv√§rinen et al.</span> make the regularity assumptions that:</p>
<div class="equation-block">
    $$\nabla_x \log p(x, \theta) q(x) \to 0 \quad \text{as} \quad \|x\|_2 \to \infty$$
</div>

<h2>Limitations and Regularity Assumptions</h2>
<p>These assumptions do not hold for general point processes (such as the Poisson process or Hawkes process), which do not necessarily decay sufficiently fast at the boundary. To alleviate this, a weight function enforcing decay can be introduced, as discussed in <span class="highlight">"Is Score Matching Suitable for Estimating Point Processes?"</span> Furthermore, nominal score matching is not suitable for autoregressive models, which is also addressed in the same work.</p>

<h2>Optimization Problem</h2>
<p>Assuming the regularity assumptions hold, the optimization problem can be rewritten as:</p>
<div class="equation-block">
    $$\theta_{\text{opt}} = \operatorname*{argmin}_{\theta} \mathbb{E}_{q} \left[ \text{Tr} \left( \nabla_x^2 p(x, \theta) \right) - \frac{1}{2} \left\| \nabla_x \log p(x, \theta) \right\|_2^2 \right]$$
</div>
<p>If the data distribution is in the model class, this optimization yields the optimal value.</p>

</body>
</html>
